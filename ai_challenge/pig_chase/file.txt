Pillow found, setting as default backend.
Cannot import tensorboard, using ConsoleVisualizer.
Im here
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 5, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 5, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 6, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (3, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 26 : Training/reward per episode -> -25.000
[-] 26 : Training/max.reward -> -1.000
[-] 26 : Training/min.reward -> -1.000
[-] 26 : Training/actions per episode -> 24.000
Q-Size: 25
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 6, 3), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 6, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (3, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (3, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (5, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 6, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 6, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 6, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 3, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 2, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 51 : Training/reward per episode -> -25.000
[-] 51 : Training/max.reward -> -1.000
[-] 51 : Training/min.reward -> -1.000
[-] 51 : Training/actions per episode -> 24.000
Q-Size: 50
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 3, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 4, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 0), (4, 4, 2), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 3), (4, 4, 3), (4, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 0), (4, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (4, 4, 1), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 0), (4, 4, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 4, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 76 : Training/reward per episode -> -25.000
[-] 76 : Training/max.reward -> -1.000
[-] 76 : Training/min.reward -> -1.000
[-] 76 : Training/actions per episode -> 24.000
Q-Size: 72
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (3, 2, 3), (3, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 2), (3, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 3, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 4, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 4, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 3), (2, 4, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 2), (2, 4, 1), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 3), (2, 4, 2), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 3, 0), (2, 4, 3), (2, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((4, 3, 3), (2, 4, 0), (2, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 3, 2), (2, 4, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 4, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 101 : Training/reward per episode -> -25.000
[-] 101 : Training/max.reward -> -1.000
[-] 101 : Training/min.reward -> -1.000
[-] 101 : Training/actions per episode -> 24.000
Q-Size: 91
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (3, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (3, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 2), (4, 6, 1), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 3), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 119 : Training/reward per episode -> -13.000
[-] 119 : Training/max.reward -> 4.000
[-] 119 : Training/min.reward -> -1.000
[-] 119 : Training/actions per episode -> 17.000
Q-Size: 108
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 2), (5, 6, 1), (6, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 5, 2), (6, 6, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 6, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 5, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 4, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 3, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (6, 2, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (5, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (5, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 2, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 3), (4, 2, 1), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 4, 2), (4, 2, 2), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 1), (4, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (3, 2, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 144 : Training/reward per episode -> -25.000
[-] 144 : Training/max.reward -> -1.000
[-] 144 : Training/min.reward -> -1.000
[-] 144 : Training/actions per episode -> 24.000
Q-Size: 130
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 5, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 4, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 0), (4, 4, 2), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 1), (4, 4, 3), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 2), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 4, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 2), (4, 4, 0), (4, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 1), (4, 4, 0), (4, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.90208
[ACT] ((5, 4, 0), (4, 4, 2), (4, 3, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.93408
[ACT] ((5, 4, 1), (4, 4, 2), (4, 3, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((5, 4, 2), (4, 4, 2), (4, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 1), (4, 4, 1), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 2), (4, 4, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 169 : Training/reward per episode -> -25.000
[-] 169 : Training/max.reward -> -1.000
[-] 169 : Training/min.reward -> -1.000
[-] 169 : Training/actions per episode -> 24.000
Q-Size: 147
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 3, 1), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 4, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 3, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 3, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 3, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 3, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 4, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 4, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 180 : Training/reward per episode -> -11.000
[-] 180 : Training/max.reward -> -1.000
[-] 180 : Training/min.reward -> -1.000
[-] 180 : Training/actions per episode -> 10.000
Q-Size: 159
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 6, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 6, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 5, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 4, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 4, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (3, 4, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 198 : Training/reward per episode -> 7.000
[-] 198 : Training/max.reward -> 24.000
[-] 198 : Training/min.reward -> -1.000
[-] 198 : Training/actions per episode -> 17.000
Q-Size: 177
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (5, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (6, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (6, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 2, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 3), (6, 2, 2), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 0), (6, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (5, 2, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (5, 2, 3), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 2, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 3, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 3, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 4, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 223 : Training/reward per episode -> -25.000
[-] 223 : Training/max.reward -> -1.000
[-] 223 : Training/min.reward -> -1.000
[-] 223 : Training/actions per episode -> 24.000
Q-Size: 201
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 3, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 3, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 3, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 3, 2), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 3, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 3, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 3, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 2, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (3, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 2), (3, 2, 1), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 1), (3, 2, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 2), (3, 2, 3), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 3, 3), (3, 2, 0), (2, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((4, 3, 2), (3, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.74208
[ACT] ((4, 3, 1), (3, 2, 2), (2, 2, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 3, 2), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 248 : Training/reward per episode -> -25.000
[-] 248 : Training/max.reward -> -1.000
[-] 248 : Training/min.reward -> -1.000
[-] 248 : Training/actions per episode -> 24.000
Q-Size: 221
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (5, 6, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 6, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 6, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 5, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 2, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 1), (4, 2, 1), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 0), (4, 2, 2), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 3), (4, 2, 2), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 2, 2), (4, 2, 0), (5, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((2, 2, 1), (4, 2, 1), (5, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((2, 2, 0), (4, 2, 2), (5, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.6064
[ACT] ((2, 2, 3), (4, 2, 3), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 2), (4, 2, 3), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -2.2633472
[ACT] ((2, 2, 1), (4, 2, 1), (5, 2, 0), 1) -2.03648
Max action: 1
[UpdateQ]-Q[x]= -2.5106432
[-] 273 : Training/reward per episode -> -25.000
[-] 273 : Training/max.reward -> -1.000
[-] 273 : Training/min.reward -> -1.000
[-] 273 : Training/actions per episode -> 24.000
Q-Size: 237
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 5, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (5, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (5, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (5, 4, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (5, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 4, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 4, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (5, 4, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 4, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 2, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 2, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 2, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 2, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 298 : Training/reward per episode -> -25.000
[-] 298 : Training/max.reward -> -1.000
[-] 298 : Training/min.reward -> -1.000
[-] 298 : Training/actions per episode -> 24.000
Q-Size: 262
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (2, 6, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (3, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (5, 4, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 4, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 6, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 3), (4, 6, 2), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[-] 323 : Training/reward per episode -> -25.000
[-] 323 : Training/max.reward -> -1.000
[-] 323 : Training/min.reward -> -1.000
[-] 323 : Training/actions per episode -> 24.000
Q-Size: 286
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (3, 2, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 3, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 3, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 3), (6, 3, 1), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 2), (6, 3, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 3, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.63968
[ACT] ((4, 3, 2), (6, 3, 0), (6, 4, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.6768
[ACT] ((4, 3, 3), (6, 3, 1), (6, 4, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((4, 3, 2), (6, 3, 2), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 3), (6, 4, 2), (7, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 4, 1), (7, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 341 : Training/reward per episode -> -18.000
[-] 341 : Training/max.reward -> -1.000
[-] 341 : Training/min.reward -> -1.000
[-] 341 : Training/actions per episode -> 17.000
Q-Size: 300
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 2, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 3, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 4, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 5, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 5, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 5, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 5, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 5, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 3), (4, 5, 3), (4, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 2), (4, 5, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 5, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 5, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 5, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 5, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.4144
[ACT] ((3, 6, 3), (4, 5, 3), (4, 6, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((3, 6, 2), (4, 5, 1), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 3), (4, 5, 2), (4, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.865216
[ACT] ((3, 6, 2), (4, 5, 2), (4, 6, 0), 2) -1.4144
Max action: 2
[UpdateQ]-Q[x]= -1.59488
[ACT] ((3, 6, 3), (4, 5, 0), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 0), (4, 5, 1), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 1), (4, 5, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 366 : Training/reward per episode -> -25.000
[-] 366 : Training/max.reward -> -1.000
[-] 366 : Training/min.reward -> -1.000
[-] 366 : Training/actions per episode -> 24.000
Q-Size: 318
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 4, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 4, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 3), (4, 2, 0), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 0), (4, 2, 1), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 3), (4, 2, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 3, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (3, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (3, 4, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 382 : Training/reward per episode -> -11.000
[-] 382 : Training/max.reward -> 4.000
[-] 382 : Training/min.reward -> -1.000
[-] 382 : Training/actions per episode -> 15.000
Q-Size: 332
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 5, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 5, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 5, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 2), (4, 6, 3), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 3), (4, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (5, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 6, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (6, 5, 0), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 5, 0), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 406 : Training/reward per episode -> 2.000
[-] 406 : Training/max.reward -> 25.000
[-] 406 : Training/min.reward -> -1.000
[-] 406 : Training/actions per episode -> 23.000
Q-Size: 355
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (3, 6, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 6, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 6, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 6, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (6, 3, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 2, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (5, 2, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 422 : Training/reward per episode -> -11.000
[-] 422 : Training/max.reward -> 4.000
[-] 422 : Training/min.reward -> -1.000
[-] 422 : Training/actions per episode -> 15.000
Q-Size: 371
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 6, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 6, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 0), (6, 6, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 3), (6, 6, 3), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 2), (6, 6, 0), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 6, 1), (6, 6, 1), (6, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((4, 6, 0), (6, 6, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 6, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (6, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 6, 0), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (6, 6, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (6, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 4, 3), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 447 : Training/reward per episode -> -25.000
[-] 447 : Training/max.reward -> -1.000
[-] 447 : Training/min.reward -> -1.000
[-] 447 : Training/actions per episode -> 24.000
Q-Size: 392
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (3, 2, 1), (2, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 0), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 2, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 2, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 2, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 0), (2, 2, 0), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 3), (2, 2, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 2, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 3, 2), (1, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (3, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (5, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 472 : Training/reward per episode -> -25.000
[-] 472 : Training/max.reward -> -1.000
[-] 472 : Training/min.reward -> -1.000
[-] 472 : Training/actions per episode -> 24.000
Q-Size: 416
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 4, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (5, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (5, 4, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 2), (5, 4, 3), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 3), (5, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 3), (5, 4, 2), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((4, 4, 2), (5, 4, 3), (6, 4, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((4, 4, 3), (5, 4, 0), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 4, 2), (5, 4, 1), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 1), (5, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 3), (6, 4, 2), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 4, 2), (6, 4, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[-] 497 : Training/reward per episode -> -25.000
[-] 497 : Training/max.reward -> -1.000
[-] 497 : Training/min.reward -> -1.000
[-] 497 : Training/actions per episode -> 24.000
Q-Size: 434
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (5, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 2, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 3, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 4, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 4, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 1), (6, 4, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 5, 2), (6, 4, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 5, 3), (6, 4, 0), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 2), (6, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 522 : Training/reward per episode -> -25.000
[-] 522 : Training/max.reward -> -1.000
[-] 522 : Training/min.reward -> -1.000
[-] 522 : Training/actions per episode -> 24.000
Q-Size: 456
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 4, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 4, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 4, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 4, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 3, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 3, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 3, 2), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 3, 3), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 3, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 3, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 3, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 3, 3), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 3), (6, 3, 0), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 2), (6, 3, 1), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 1), (6, 3, 2), (6, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 1), (6, 3, 3), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 2), (6, 3, 0), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 3), (6, 3, 1), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 2), (6, 3, 1), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 6, 3), (6, 3, 3), (6, 2, 0), 0) -1.312
Max action: 0
[UpdateQ]-Q[x]= -2.00448
[ACT] ((3, 6, 3), (6, 3, 0), (6, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((3, 6, 2), (6, 3, 1), (6, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((3, 6, 1), (6, 3, 1), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.74208
[ACT] ((4, 6, 1), (6, 3, 3), (6, 2, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.03648
[-] 547 : Training/reward per episode -> -25.000
[-] 547 : Training/max.reward -> -1.000
[-] 547 : Training/min.reward -> -1.000
[-] 547 : Training/actions per episode -> 24.000
Q-Size: 470
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 1), (6, 4, 2), (5, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 0), (6, 4, 0), (5, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 1), (6, 4, 1), (5, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 0), (6, 4, 2), (5, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 1), (6, 4, 3), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 4, 0), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 4, 0), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 572 : Training/reward per episode -> -25.000
[-] 572 : Training/max.reward -> -1.000
[-] 572 : Training/min.reward -> -1.000
[-] 572 : Training/actions per episode -> 24.000
Q-Size: 491
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (3, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (3, 2, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (3, 2, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (3, 2, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (3, 2, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (3, 2, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 579 : Training/reward per episode -> -2.000
[-] 579 : Training/max.reward -> 4.000
[-] 579 : Training/min.reward -> -1.000
[-] 579 : Training/actions per episode -> 6.000
Q-Size: 498
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 5, 2), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 5, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 5, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 6, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 3), (2, 6, 1), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 2), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 1), (4, 6, 3), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[-] 604 : Training/reward per episode -> -25.000
[-] 604 : Training/max.reward -> -1.000
[-] 604 : Training/min.reward -> -1.000
[-] 604 : Training/actions per episode -> 24.000
Q-Size: 520
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 5, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 5, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 5, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 5, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 5, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 5, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 5, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 5, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 5, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 3), (2, 2, 0), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 2), (2, 2, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (3, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 629 : Training/reward per episode -> -25.000
[-] 629 : Training/max.reward -> -1.000
[-] 629 : Training/min.reward -> -1.000
[-] 629 : Training/actions per episode -> 24.000
Q-Size: 544
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (5, 2, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (5, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (6, 2, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (6, 3, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((2, 2, 0), (6, 4, 0), (6, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.7088
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -2.0413952
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) -1.63968
Max action: 2
[UpdateQ]-Q[x]= -1.127936
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.893632
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -1.7088
Max action: 2
[UpdateQ]-Q[x]= -2.448252928
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -2.0413952
Max action: 1
[UpdateQ]-Q[x]= -1.93015808
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) -1.127936
Max action: 2
[UpdateQ]-Q[x]= -2.23751168
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) -1.893632
Max action: 1
[UpdateQ]-Q[x]= -1.1787264
[ACT] ((2, 2, 3), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 3), (6, 4, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.714384896
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) -1.1787264
Max action: 1
[UpdateQ]-Q[x]= -1.790130176
[-] 654 : Training/reward per episode -> -25.000
[-] 654 : Training/max.reward -> -1.000
[-] 654 : Training/min.reward -> -1.000
[-] 654 : Training/actions per episode -> 24.000
Q-Size: 557
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 2, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 3, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 3, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (4, 6, 1), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (4, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (4, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 3), (4, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 6, 0), (4, 6, 1), (3, 6, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((6, 6, 3), (4, 6, 2), (3, 6, 0), 0) -1.472
Max action: 0
[UpdateQ]-Q[x]= -1.6064
[ACT] ((5, 6, 3), (4, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 0), (3, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 6, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 6, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (3, 6, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 6, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (3, 6, 0), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 6, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 679 : Training/reward per episode -> -25.000
[-] 679 : Training/max.reward -> -1.000
[-] 679 : Training/min.reward -> -1.000
[-] 679 : Training/actions per episode -> 24.000
Q-Size: 575
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 4, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (6, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 2, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (3, 2, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 2, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 3, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 4, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 4, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (3, 4, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 4, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 1), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 4, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 4, 3), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 702 : Training/reward per episode -> -18.000
[-] 702 : Training/max.reward -> 4.000
[-] 702 : Training/min.reward -> -1.000
[-] 702 : Training/actions per episode -> 22.000
Q-Size: 598
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (5, 6, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (3, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 6, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 5, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 5, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 5, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 717 : Training/reward per episode -> 10.000
[-] 717 : Training/max.reward -> 24.000
[-] 717 : Training/min.reward -> -1.000
[-] 717 : Training/actions per episode -> 14.000
Q-Size: 613
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (2, 3, 0), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 3, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 3, 0), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 3, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 3, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 3, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 3, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 3, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 3, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 3, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 3, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 3, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 3, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 3, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 3, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 3, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 3, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 3, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 3, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 3, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 2, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 742 : Training/reward per episode -> -25.000
[-] 742 : Training/max.reward -> -1.000
[-] 742 : Training/min.reward -> -1.000
[-] 742 : Training/actions per episode -> 24.000
Q-Size: 638
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 751 : Training/reward per episode -> -4.000
[-] 751 : Training/max.reward -> 4.000
[-] 751 : Training/min.reward -> -1.000
[-] 751 : Training/actions per episode -> 8.000
Q-Size: 647
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 4, 0), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 4, 0), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 4, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 4, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 4, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 4, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 5, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 6, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 6, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 6, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 6, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 6, 1), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 6, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 6, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 6, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 6, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 6, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (6, 6, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 776 : Training/reward per episode -> -25.000
[-] 776 : Training/max.reward -> -1.000
[-] 776 : Training/min.reward -> -1.000
[-] 776 : Training/actions per episode -> 24.000
Q-Size: 671
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 3, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 4, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 4, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 6, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 5, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 4, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 801 : Training/reward per episode -> -25.000
[-] 801 : Training/max.reward -> -1.000
[-] 801 : Training/min.reward -> -1.000
[-] 801 : Training/actions per episode -> 24.000
Q-Size: 696
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 2, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 2, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 2, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 2, 3), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 2, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 2, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 3, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 4, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 4, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 4, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 4, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 3, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 826 : Training/reward per episode -> -25.000
[-] 826 : Training/max.reward -> -1.000
[-] 826 : Training/min.reward -> -1.000
[-] 826 : Training/actions per episode -> 24.000
Q-Size: 721
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 2), (4, 6, 1), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 3), (4, 6, 2), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 0), (4, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((5, 4, 1), (4, 6, 0), (5, 6, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -2.00448
[ACT] ((5, 4, 2), (4, 6, 1), (5, 6, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.7088
[ACT] ((5, 4, 3), (4, 6, 2), (5, 6, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -0.992
[ACT] ((5, 4, 0), (4, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 6, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (6, 6, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 6, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 6, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 2), (6, 6, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 3), (6, 6, 0), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 0), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 6, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.74208
[ACT] ((5, 4, 2), (6, 6, 3), (6, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.03648
[-] 851 : Training/reward per episode -> -25.000
[-] 851 : Training/max.reward -> -1.000
[-] 851 : Training/min.reward -> -1.000
[-] 851 : Training/actions per episode -> 24.000
Q-Size: 738
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 5, 1), (4, 4, 0), 0) 0
[ACT] ((6, 6, 0), (2, 5, 1), (4, 4, 0), 2) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 5, 1), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 5, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 4, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 4, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (3, 4, 1), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 4, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 860 : Training/reward per episode -> -4.000
[-] 860 : Training/max.reward -> 4.000
[-] 860 : Training/min.reward -> -1.000
[-] 860 : Training/actions per episode -> 8.000
Q-Size: 747
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (3, 2, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 0), (3, 2, 1), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 5, 3), (4, 2, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 3, 2), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 3, 2), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 868 : Training/reward per episode -> 18.000
[-] 868 : Training/max.reward -> 25.000
[-] 868 : Training/min.reward -> -1.000
[-] 868 : Training/actions per episode -> 7.000
Q-Size: 754
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 5, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 4, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 4, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (3, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (3, 4, 0), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (3, 4, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 884 : Training/reward per episode -> 9.000
[-] 884 : Training/max.reward -> 24.000
[-] 884 : Training/min.reward -> -1.000
[-] 884 : Training/actions per episode -> 15.000
Q-Size: 769
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (6, 2, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (6, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (5, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 2, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 5, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 5, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 894 : Training/reward per episode -> 16.000
[-] 894 : Training/max.reward -> 25.000
[-] 894 : Training/min.reward -> -1.000
[-] 894 : Training/actions per episode -> 9.000
Q-Size: 779
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (3, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (3, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 4, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 6, 3), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 4, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 4, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 4, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (6, 4, 1), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 3), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 4, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 919 : Training/reward per episode -> -25.000
[-] 919 : Training/max.reward -> -1.000
[-] 919 : Training/min.reward -> -1.000
[-] 919 : Training/actions per episode -> 24.000
Q-Size: 803
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 4, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (5, 4, 3), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 3), (5, 4, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (5, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (5, 4, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (5, 4, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 1), (5, 4, 3), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 2, 0), (5, 4, 0), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 1), (5, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 944 : Training/reward per episode -> -25.000
[-] 944 : Training/max.reward -> -1.000
[-] 944 : Training/min.reward -> -1.000
[-] 944 : Training/actions per episode -> 24.000
Q-Size: 825
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 4, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 4, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 949 : Training/reward per episode -> 0.000
[-] 949 : Training/max.reward -> 4.000
[-] 949 : Training/min.reward -> -1.000
[-] 949 : Training/actions per episode -> 4.000
Q-Size: 830
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (6, 6, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (6, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (6, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (6, 4, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 4, 3), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 4, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 4, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 3, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 0), (4, 2, 0), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 4, 3), (4, 2, 0), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 4, 2), (4, 2, 2), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 4, 1), (4, 2, 3), (3, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((6, 4, 0), (4, 2, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 974 : Training/reward per episode -> -25.000
[-] 974 : Training/max.reward -> -1.000
[-] 974 : Training/min.reward -> -1.000
[-] 974 : Training/actions per episode -> 24.000
Q-Size: 851
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 6, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 5, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 4, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 4, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 0), (4, 4, 2), (5, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 1), (4, 4, 3), (5, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 0), (4, 4, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (5, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (5, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 20.0
[-] 998 : Training/reward per episode -> 2.000
[-] 998 : Training/max.reward -> 25.000
[-] 998 : Training/min.reward -> -1.000
[-] 998 : Training/actions per episode -> 23.000
Q-Size: 873
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 6, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 5, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 4, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 2, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1023 : Training/reward per episode -> -25.000
[-] 1023 : Training/max.reward -> -1.000
[-] 1023 : Training/min.reward -> -1.000
[-] 1023 : Training/actions per episode -> 24.000
Q-Size: 898
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (5, 6, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 6, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 6, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (6, 5, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (6, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (5, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 4, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 3, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (2, 3, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 4, 2), (2, 3, 3), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 3), (2, 3, 0), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 0), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.74208
[ACT] ((2, 4, 2), (2, 3, 3), (2, 2, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.7088
[ACT] ((2, 4, 3), (2, 3, 0), (2, 2, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((2, 4, 0), (2, 3, 0), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1048 : Training/reward per episode -> -25.000
[-] 1048 : Training/max.reward -> -1.000
[-] 1048 : Training/min.reward -> -1.000
[-] 1048 : Training/actions per episode -> 24.000
Q-Size: 918
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (5, 2, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 3), (4, 2, 2), (4, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (4, 3, 2), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 3), (4, 4, 2), (4, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 3), (4, 5, 2), (4, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (4, 5, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 4, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (4, 4, 2), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 2), (4, 4, 3), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 4, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (4, 4, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 0), (4, 4, 3), (4, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 0), (4, 4, 0), (4, 1, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 3, 0), (4, 1, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 3, 0), (4, 1, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 2, 0), (4, 1, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1073 : Training/reward per episode -> -25.000
[-] 1073 : Training/max.reward -> -1.000
[-] 1073 : Training/min.reward -> -1.000
[-] 1073 : Training/actions per episode -> 24.000
Q-Size: 936
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 2, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 3, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.74208
[ACT] ((2, 5, 2), (6, 4, 2), (6, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.0944
[ACT] ((2, 5, 3), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (5, 6, 3), (2, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 6, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (3, 6, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 6, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 6, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 5, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 4, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (2, 4, 1), (3, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 3), (2, 4, 1), (3, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1098 : Training/reward per episode -> -25.000
[-] 1098 : Training/max.reward -> -1.000
[-] 1098 : Training/min.reward -> -1.000
[-] 1098 : Training/actions per episode -> 24.000
Q-Size: 958
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (6, 4, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (6, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (5, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 4, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1107 : Training/reward per episode -> 16.000
[-] 1107 : Training/max.reward -> 24.000
[-] 1107 : Training/min.reward -> -1.000
[-] 1107 : Training/actions per episode -> 8.000
Q-Size: 967
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 5, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 5, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 4, 3), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (4, 4, 1), (5, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (4, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (4, 4, 3), (5, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 3), (4, 4, 0), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 4, 0), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 4, 3), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 1125 : Training/reward per episode -> -13.000
[-] 1125 : Training/max.reward -> 4.000
[-] 1125 : Training/min.reward -> -1.000
[-] 1125 : Training/actions per episode -> 17.000
Q-Size: 983
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (5, 4, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (5, 4, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (5, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (5, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (5, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1134 : Training/reward per episode -> -9.000
[-] 1134 : Training/max.reward -> -1.000
[-] 1134 : Training/min.reward -> -1.000
[-] 1134 : Training/actions per episode -> 8.000
Q-Size: 992
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 5, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (3, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 4, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (6, 4, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 5, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (6, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (6, 6, 3), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 0), (6, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (6, 6, 1), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 2, 0), (6, 6, 1), (5, 6, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.6768
[ACT] ((6, 2, 1), (6, 6, 3), (5, 6, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((6, 2, 0), (6, 6, 0), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1159 : Training/reward per episode -> -25.000
[-] 1159 : Training/max.reward -> -1.000
[-] 1159 : Training/min.reward -> -1.000
[-] 1159 : Training/actions per episode -> 24.000
Q-Size: 1012
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (3, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (4, 6, 3), (5, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 2), (4, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 0), (4, 6, 1), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 4, 3), (4, 6, 2), (5, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 4, 3), (4, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 3), (4, 6, 2), (5, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 3), (4, 6, 0), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 2), (4, 6, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (4, 6, 2), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 5, 1), (4, 6, 3), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1184 : Training/reward per episode -> -25.000
[-] 1184 : Training/max.reward -> -1.000
[-] 1184 : Training/min.reward -> -1.000
[-] 1184 : Training/actions per episode -> 24.000
Q-Size: 1030
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 0), (2, 6, 2), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 3), (2, 6, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 3), (2, 6, 1), (5, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 3), (3, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 1196 : Training/reward per episode -> -7.000
[-] 1196 : Training/max.reward -> 4.000
[-] 1196 : Training/min.reward -> -1.000
[-] 1196 : Training/actions per episode -> 11.000
Q-Size: 1040
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 3, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (5, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 1), (6, 4, 2), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 2, 1), (6, 4, 0), (6, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.03648
[ACT] ((6, 2, 1), (6, 4, 2), (6, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.2461952
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -1.79968
Max action: 2
[UpdateQ]-Q[x]= -1.159936
[ACT] ((6, 2, 1), (6, 4, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -2.1033472
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) -2.03648
Max action: 2
[UpdateQ]-Q[x]= -2.644860928
[ACT] ((6, 2, 1), (6, 4, 2), (6, 5, 0), 1) -2.2461952
Max action: 1
[UpdateQ]-Q[x]= -1.99159808
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -1.159936
Max action: 2
[UpdateQ]-Q[x]= -2.3148544
[ACT] ((6, 2, 1), (6, 4, 0), (6, 5, 0), 1) -2.00448
Max action: 1
[UpdateQ]-Q[x]= -2.89360699392
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) -2.644860928
Max action: 2
[UpdateQ]-Q[x]= -2.6035949568
[ACT] ((6, 2, 1), (6, 4, 2), (6, 5, 0), 1) -1.99159808
Max action: 1
[UpdateQ]-Q[x]= -1.198319616
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1221 : Training/reward per episode -> -25.000
[-] 1221 : Training/max.reward -> -1.000
[-] 1221 : Training/min.reward -> -1.000
[-] 1221 : Training/actions per episode -> 24.000
Q-Size: 1052
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (5, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 3, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 4, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 3), (2, 4, 2), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (2, 4, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 3), (2, 4, 0), (2, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 3), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 4, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 3), (2, 4, 1), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 2), (2, 4, 2), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 1), (2, 4, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 2), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1246 : Training/reward per episode -> -25.000
[-] 1246 : Training/max.reward -> -1.000
[-] 1246 : Training/min.reward -> -1.000
[-] 1246 : Training/actions per episode -> 24.000
Q-Size: 1071
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (6, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (6, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (5, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (3, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 2, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 2, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 1), (2, 2, 0), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 4, 2), (2, 2, 1), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 4, 1), (2, 2, 2), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 0), (2, 3, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 4, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 4, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (3, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1271 : Training/reward per episode -> -25.000
[-] 1271 : Training/max.reward -> -1.000
[-] 1271 : Training/min.reward -> -1.000
[-] 1271 : Training/actions per episode -> 24.000
Q-Size: 1093
Random Choice
[UpdateQ]-Q[x]= -0.8
[-] 1272 : Training/reward per episode -> -1.000
[-] 1272 : Training/max.reward -> -1.000
[-] 1272 : Training/min.reward -> -1.000
[-] 1272 : Training/actions per episode -> 0.000
Q-Size: 1095
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (3, 2, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 3, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 3, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 1281 : Training/reward per episode -> 17.000
[-] 1281 : Training/max.reward -> 25.000
[-] 1281 : Training/min.reward -> -1.000
[-] 1281 : Training/actions per episode -> 8.000
Q-Size: 1104
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 5, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 4, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (3, 4, 3), (1, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 4, 3), (1, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 1289 : Training/reward per episode -> 18.000
[-] 1289 : Training/max.reward -> 25.000
[-] 1289 : Training/min.reward -> -1.000
[-] 1289 : Training/actions per episode -> 7.000
Q-Size: 1112
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 6, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 5, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (2, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 6, 1), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (3, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (3, 6, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (3, 6, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (3, 6, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 6, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (4, 6, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 6, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 4, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1310 : Training/reward per episode -> 4.000
[-] 1310 : Training/max.reward -> 24.000
[-] 1310 : Training/min.reward -> -1.000
[-] 1310 : Training/actions per episode -> 20.000
Q-Size: 1133
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (3, 2, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (3, 2, 0), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 2, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (3, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 3), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (3, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 2, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (3, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (5, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1335 : Training/reward per episode -> -25.000
[-] 1335 : Training/max.reward -> -1.000
[-] 1335 : Training/min.reward -> -1.000
[-] 1335 : Training/actions per episode -> 24.000
Q-Size: 1158
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 1339 : Training/reward per episode -> 22.000
[-] 1339 : Training/max.reward -> 25.000
[-] 1339 : Training/min.reward -> -1.000
[-] 1339 : Training/actions per episode -> 3.000
Q-Size: 1162
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (3, 4, 3), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (3, 4, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 1), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 3), (6, 4, 1), (6, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 2), (6, 4, 2), (6, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 3), (6, 5, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 5, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (6, 5, 0), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 5, 1), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 5, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 6, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (5, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (5, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 5, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 1), (2, 4, 0), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 2), (2, 4, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 4, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1364 : Training/reward per episode -> -25.000
[-] 1364 : Training/max.reward -> -1.000
[-] 1364 : Training/min.reward -> -1.000
[-] 1364 : Training/actions per episode -> 24.000
Q-Size: 1184
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 4, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (6, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 2, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 2, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 3), (2, 2, 0), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 2), (2, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 1), (2, 2, 2), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 2), (2, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((3, 2, 2), (2, 2, 1), (2, 3, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((3, 2, 1), (2, 2, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (2, 3, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1389 : Training/reward per episode -> -25.000
[-] 1389 : Training/max.reward -> -1.000
[-] 1389 : Training/min.reward -> -1.000
[-] 1389 : Training/actions per episode -> 24.000
Q-Size: 1206
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 6, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 6, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 5, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 4, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 1397 : Training/reward per episode -> 18.000
[-] 1397 : Training/max.reward -> 25.000
[-] 1397 : Training/min.reward -> -1.000
[-] 1397 : Training/actions per episode -> 7.000
Q-Size: 1214
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 6, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 6, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 6, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 6, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (2, 6, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 5, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (3, 2, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 2, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 2, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 3, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 6, 2), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (5, 6, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1422 : Training/reward per episode -> -25.000
[-] 1422 : Training/max.reward -> -1.000
[-] 1422 : Training/min.reward -> -1.000
[-] 1422 : Training/actions per episode -> 24.000
Q-Size: 1239
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 5, 0), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 5, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (2, 4, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (2, 3, 0), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 3, 0), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 3, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 3, 3), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 3, 3), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 3, 3), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 3, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 1433 : Training/reward per episode -> -6.000
[-] 1433 : Training/max.reward -> 4.000
[-] 1433 : Training/min.reward -> -1.000
[-] 1433 : Training/actions per episode -> 10.000
Q-Size: 1250
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 4, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 3, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 3, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 3, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 3, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 3, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 3, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 3, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 2, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 2, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (5, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 2, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (5, 2, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 2, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 2, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 2, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (3, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (3, 2, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1458 : Training/reward per episode -> -25.000
[-] 1458 : Training/max.reward -> -1.000
[-] 1458 : Training/min.reward -> -1.000
[-] 1458 : Training/actions per episode -> 24.000
Q-Size: 1275
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (5, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (6, 6, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (6, 6, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 6, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (6, 6, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 2), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 0), (6, 6, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 1), (6, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (6, 6, 3), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 1), (6, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 5, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1476 : Training/reward per episode -> 7.000
[-] 1476 : Training/max.reward -> 24.000
[-] 1476 : Training/min.reward -> -1.000
[-] 1476 : Training/actions per episode -> 17.000
Q-Size: 1290
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (3, 2, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 2, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 2, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 3, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 3, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 4, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (3, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (3, 4, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (3, 4, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (3, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (3, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (3, 6, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 3), (4, 6, 0), (4, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 2, 2), (4, 6, 0), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 3), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1501 : Training/reward per episode -> -25.000
[-] 1501 : Training/max.reward -> -1.000
[-] 1501 : Training/min.reward -> -1.000
[-] 1501 : Training/actions per episode -> 24.000
Q-Size: 1313
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 2, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 2, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 2, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 4, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (3, 2, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (3, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (2, 2, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 2, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 3, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 20.0
[-] 1523 : Training/reward per episode -> 4.000
[-] 1523 : Training/max.reward -> 25.000
[-] 1523 : Training/min.reward -> -1.000
[-] 1523 : Training/actions per episode -> 21.000
Q-Size: 1335
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 4, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 4, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (2, 4, 2), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 0), (2, 5, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 5, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 5, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 5, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 5, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (2, 5, 0), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 0), (2, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 4, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 5, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 5, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 5, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 5, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 5, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 4, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 4, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 4, 2), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 4, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 4, 2), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (2, 5, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (2, 5, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1548 : Training/reward per episode -> -25.000
[-] 1548 : Training/max.reward -> -1.000
[-] 1548 : Training/min.reward -> -1.000
[-] 1548 : Training/actions per episode -> 24.000
Q-Size: 1358
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 3), (6, 6, 2), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 2, 2), (6, 6, 3), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 1), (5, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (5, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (5, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 5, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 6, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 6, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 6, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 5, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 2, 2), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1573 : Training/reward per episode -> -25.000
[-] 1573 : Training/max.reward -> -1.000
[-] 1573 : Training/min.reward -> -1.000
[-] 1573 : Training/actions per episode -> 24.000
Q-Size: 1381
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 6, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 5, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 1), (2, 2, 3), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 2), (2, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 1), (2, 2, 1), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 4, 2), (2, 2, 2), (3, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.6768
[ACT] ((6, 4, 1), (2, 2, 3), (3, 2, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.83168
[ACT] ((6, 4, 2), (2, 2, 0), (3, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.2141952
[ACT] ((6, 4, 1), (2, 2, 1), (3, 2, 0), 2) -1.79968
Max action: 2
[UpdateQ]-Q[x]= -2.233088
[ACT] ((6, 4, 2), (2, 2, 2), (3, 2, 0), 1) -1.6768
Max action: 1
[UpdateQ]-Q[x]= -2.3076352
[ACT] ((6, 4, 1), (2, 2, 3), (3, 2, 0), 2) -1.83168
Max action: 2
[UpdateQ]-Q[x]= -2.583420928
[ACT] ((6, 4, 2), (2, 2, 0), (3, 2, 0), 1) -2.2141952
Max action: 1
[UpdateQ]-Q[x]= -2.67201536
[ACT] ((6, 4, 1), (2, 2, 1), (3, 2, 0), 2) -2.233088
Max action: 2
[UpdateQ]-Q[x]= -2.723504128
[ACT] ((6, 4, 2), (2, 2, 2), (3, 2, 0), 1) -2.3076352
Max action: 1
[UpdateQ]-Q[x]= -2.91491643392
[ACT] ((6, 4, 1), (2, 2, 3), (3, 2, 0), 2) -2.583420928
Max action: 2
[UpdateQ]-Q[x]= -2.97007357952
[-] 1598 : Training/reward per episode -> -25.000
[-] 1598 : Training/max.reward -> -1.000
[-] 1598 : Training/min.reward -> -1.000
[-] 1598 : Training/actions per episode -> 24.000
Q-Size: 1394
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 3, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 3, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 3, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 3, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 3, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 3, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (4, 3, 1), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 3), (4, 3, 1), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 0), (4, 3, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 3), (4, 3, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 3, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 3, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 3, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 3, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 3, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 3, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 3, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 2, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 2, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 2, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 2, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1623 : Training/reward per episode -> -25.000
[-] 1623 : Training/max.reward -> -1.000
[-] 1623 : Training/min.reward -> -1.000
[-] 1623 : Training/actions per episode -> 24.000
Q-Size: 1416
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 4, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 4, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 5, 3), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 5, 0), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 5, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 5, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 5, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 5, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 6, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 0), (4, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 1), (4, 6, 3), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 0), (4, 6, 0), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1648 : Training/reward per episode -> -25.000
[-] 1648 : Training/max.reward -> -1.000
[-] 1648 : Training/min.reward -> -1.000
[-] 1648 : Training/actions per episode -> 24.000
Q-Size: 1438
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (3, 6, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 6, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 5, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 5, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 4, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 4, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 2, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (3, 2, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 2, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 2, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (3, 2, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (3, 2, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 2, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (3, 2, 3), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (3, 2, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 1673 : Training/reward per episode -> -25.000
[-] 1673 : Training/max.reward -> -1.000
[-] 1673 : Training/min.reward -> -1.000
[-] 1673 : Training/actions per episode -> 24.000
Q-Size: 1463
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 6, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (6, 5, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 4, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (6, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (5, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 2, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1686 : Training/reward per episode -> 12.000
[-] 1686 : Training/max.reward -> 24.000
[-] 1686 : Training/min.reward -> -1.000
[-] 1686 : Training/actions per episode -> 12.000
Q-Size: 1476
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 3, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 4, 3), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1690 : Training/reward per episode -> 21.000
[-] 1690 : Training/max.reward -> 24.000
[-] 1690 : Training/min.reward -> -1.000
[-] 1690 : Training/actions per episode -> 3.000
Q-Size: 1480
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 5, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 5, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (3, 4, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 4, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.500416
[ACT] ((4, 3, 3), (2, 4, 0), (2, 5, 0), 1) -1.0944
Max action: 1
[UpdateQ]-Q[x]= -1.53088
[ACT] ((4, 3, 2), (2, 4, 0), (2, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 4, 2), (2, 4, 2), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 3), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 4, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.4144
[ACT] ((4, 4, 2), (2, 4, 2), (2, 5, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((4, 4, 3), (2, 4, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 4, 0), (2, 4, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.865216
[ACT] ((4, 4, 3), (2, 4, 1), (2, 5, 0), 1) -1.4144
Max action: 1
[UpdateQ]-Q[x]= -2.04544
[ACT] ((4, 4, 2), (2, 4, 2), (2, 5, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= -1.6128
[ACT] ((4, 4, 3), (2, 4, 2), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -2.15373824
[ACT] ((4, 4, 0), (2, 4, 0), (2, 5, 0), 1) -1.865216
Max action: 1
[UpdateQ]-Q[x]= -2.36678144
[-] 1715 : Training/reward per episode -> -25.000
[-] 1715 : Training/max.reward -> -1.000
[-] 1715 : Training/min.reward -> -1.000
[-] 1715 : Training/actions per episode -> 24.000
Q-Size: 1495
[ACT] ((3, 2, 2), (3, 6, 1), (5, 4, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.0624
[ACT] ((3, 2, 3), (3, 6, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (3, 6, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (3, 6, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 6, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 6, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (4, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 3), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 6, 1), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 6, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 6, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 1), (6, 6, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 3, 2), (6, 6, 2), (6, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((2, 3, 1), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 6, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 6, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 6, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1740 : Training/reward per episode -> -25.000
[-] 1740 : Training/max.reward -> -1.000
[-] 1740 : Training/min.reward -> -1.000
[-] 1740 : Training/actions per episode -> 24.000
Q-Size: 1516
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 5, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 5, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 5, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 4, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 4, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (4, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.76026624
[ACT] ((4, 3, 2), (3, 4, 3), (2, 5, 0), 2) -1.500416
Max action: 2
[UpdateQ]-Q[x]= -1.1000832
[ACT] ((4, 3, 3), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 0), (2, 4, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 1), (2, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 2), (2, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 1765 : Training/reward per episode -> -25.000
[-] 1765 : Training/max.reward -> -1.000
[-] 1765 : Training/min.reward -> -1.000
[-] 1765 : Training/actions per episode -> 24.000
Q-Size: 1537
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 6, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 6, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 3, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 2, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 2, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (3, 2, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 2, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (5, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 2, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 2, 3), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (5, 2, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 2, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 2, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 3, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (2, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 1789 : Training/reward per episode -> 1.000
[-] 1789 : Training/max.reward -> 24.000
[-] 1789 : Training/min.reward -> -1.000
[-] 1789 : Training/actions per episode -> 23.000
Q-Size: 1561
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 2), (2, 5, 1), (6, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 1), (2, 4, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 4, 0), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 4, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (3, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (3, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (5, 4, 1), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 1802 : Training/reward per episode -> 13.000
[-] 1802 : Training/max.reward -> 25.000
[-] 1802 : Training/min.reward -> -1.000
[-] 1802 : Training/actions per episode -> 12.000
Q-Size: 1573
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (4, 2, 3), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 2), (3, 2, 3), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 3), (3, 2, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.4144
[ACT] ((3, 2, 3), (2, 2, 0), (2, 3, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.671936
[ACT] ((3, 2, 2), (2, 2, 1), (2, 3, 0), 1) -1.0624
Max action: 1
[UpdateQ]-Q[x]= -1.62688
[ACT] ((3, 2, 1), (2, 2, 2), (2, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((3, 2, 2), (2, 2, 3), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -2.0093952
[ACT] ((3, 2, 1), (2, 2, 0), (2, 3, 0), 2) -1.63968
Max action: 2
[UpdateQ]-Q[x]= -1.127936
[ACT] ((3, 2, 2), (2, 2, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (2, 2, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 3, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 1), (2, 4, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 0), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1827 : Training/reward per episode -> -25.000
[-] 1827 : Training/max.reward -> -1.000
[-] 1827 : Training/min.reward -> -1.000
[-] 1827 : Training/actions per episode -> 24.000
Q-Size: 1590
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 6, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 6, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 6, 2), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 6, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 6, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 6, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 2), (6, 6, 2), (4, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 3), (6, 6, 2), (4, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (6, 6, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 6, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 2), (6, 6, 1), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 1), (6, 6, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 6, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 6, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 6, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 6, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (5, 6, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 6, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 6, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (5, 6, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 6, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 1852 : Training/reward per episode -> -25.000
[-] 1852 : Training/max.reward -> -1.000
[-] 1852 : Training/min.reward -> -1.000
[-] 1852 : Training/actions per episode -> 24.000
Q-Size: 1612
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 2, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (3, 2, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 3), (3, 2, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 2), (3, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (3, 2, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 3), (3, 2, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 0), (3, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (3, 2, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 2, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 2, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (2, 2, 3), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 0), (2, 2, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 2, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 1877 : Training/reward per episode -> -25.000
[-] 1877 : Training/max.reward -> -1.000
[-] 1877 : Training/min.reward -> -1.000
[-] 1877 : Training/actions per episode -> 24.000
Q-Size: 1634
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 2, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 3, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (5, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 4, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 5, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.4144
[ACT] ((5, 6, 3), (4, 5, 2), (4, 6, 0), 0) -0.96
Max action: 0
[UpdateQ]-Q[x]= -0.992
[ACT] ((4, 6, 3), (4, 5, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 0), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 5, 2), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 5, 2), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 6, 3), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 6, 3), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (3, 6, 0), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1902 : Training/reward per episode -> -25.000
[-] 1902 : Training/max.reward -> -1.000
[-] 1902 : Training/min.reward -> -1.000
[-] 1902 : Training/actions per episode -> 24.000
Q-Size: 1658
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 4, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (3, 4, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 1), (2, 4, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 6, 0), (2, 4, 1), (2, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((6, 6, 3), (2, 4, 2), (2, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.6064
[ACT] ((6, 6, 2), (2, 4, 2), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -2.1117952
[ACT] ((6, 6, 1), (2, 4, 0), (2, 5, 0), 1) -1.79968
Max action: 1
[UpdateQ]-Q[x]= -2.4428032
[ACT] ((6, 6, 0), (2, 4, 1), (2, 5, 0), 1) -2.00448
Max action: 1
[UpdateQ]-Q[x]= -1.200896
[ACT] ((6, 6, 3), (2, 4, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (2, 4, 1), (2, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (2, 4, 2), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -2.311548928
[ACT] ((6, 6, 2), (2, 4, 2), (2, 5, 0), 1) -2.1117952
Max action: 1
[UpdateQ]-Q[x]= -2.785753088
[ACT] ((6, 6, 1), (2, 4, 0), (2, 5, 0), 1) -2.4428032
Max action: 1
[UpdateQ]-Q[x]= -2.05713408
[ACT] ((6, 6, 0), (2, 4, 1), (2, 5, 0), 1) -1.200896
Max action: 1
[UpdateQ]-Q[x]= -2.0682752
[ACT] ((6, 6, 3), (2, 4, 2), (2, 5, 0), 1) -1.6064
Max action: 1
[UpdateQ]-Q[x]= -2.90416197632
[ACT] ((6, 6, 2), (2, 4, 2), (2, 5, 0), 1) -2.785753088
Max action: 1
[UpdateQ]-Q[x]= -2.6737164288
[ACT] ((6, 6, 1), (2, 4, 0), (2, 5, 0), 1) -2.05713408
Max action: 1
[UpdateQ]-Q[x]= -2.535122944
[ACT] ((6, 6, 0), (2, 4, 1), (2, 5, 0), 1) -2.0682752
Max action: 1
[UpdateQ]-Q[x]= -3.07231870484
[ACT] ((6, 6, 3), (2, 4, 2), (2, 5, 0), 1) -2.90416197632
Max action: 1
[UpdateQ]-Q[x]= -1.38083239526
[ACT] ((6, 6, 2), (2, 5, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 1927 : Training/reward per episode -> -25.000
[-] 1927 : Training/max.reward -> -1.000
[-] 1927 : Training/min.reward -> -1.000
[-] 1927 : Training/actions per episode -> 24.000
Q-Size: 1667
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 5, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 5, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 5, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 4, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 3, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 3, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 3, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 2, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 2, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (3, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (3, 2, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (3, 2, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (3, 2, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (3, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 1), (3, 2, 3), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 2), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 2, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (2, 2, 3), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 1), (2, 2, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 1952 : Training/reward per episode -> -25.000
[-] 1952 : Training/max.reward -> -1.000
[-] 1952 : Training/min.reward -> -1.000
[-] 1952 : Training/actions per episode -> 24.000
Q-Size: 1690
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (5, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 2), (4, 4, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 1), (3, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (3, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 2), (6, 4, 3), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 3), (6, 4, 0), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 2), (6, 4, 1), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 1), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 2), (6, 4, 1), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (6, 4, 2), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[-] 1977 : Training/reward per episode -> -25.000
[-] 1977 : Training/max.reward -> -1.000
[-] 1977 : Training/min.reward -> -1.000
[-] 1977 : Training/actions per episode -> 24.000
Q-Size: 1709
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 3, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 3, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 3, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 3, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 3, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (2, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (2, 4, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 4, 1), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (2, 4, 3), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (2, 4, 0), (3, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.90208
[ACT] ((2, 3, 3), (2, 4, 1), (3, 4, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.6064
[ACT] ((2, 3, 0), (2, 4, 2), (3, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 3, 3), (2, 4, 2), (3, 4, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -2.2797312
[ACT] ((2, 3, 0), (2, 4, 0), (3, 4, 0), 1) -1.90208
Max action: 1
[UpdateQ]-Q[x]= -2.208512
[ACT] ((2, 3, 3), (2, 4, 1), (3, 4, 0), 2) -1.6064
Max action: 2
[UpdateQ]-Q[x]= -2.2730752
[ACT] ((2, 3, 0), (2, 4, 2), (3, 4, 0), 1) -1.79968
Max action: 1
[UpdateQ]-Q[x]= -1.671936
[ACT] ((2, 3, 3), (2, 4, 3), (3, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 0), (2, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (3, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2002 : Training/reward per episode -> -25.000
[-] 2002 : Training/max.reward -> -1.000
[-] 2002 : Training/min.reward -> -1.000
[-] 2002 : Training/actions per episode -> 24.000
Q-Size: 1726
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (2, 3, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 4, 1), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (3, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (5, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 4, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (5, 4, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (5, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2027 : Training/reward per episode -> -25.000
[-] 2027 : Training/max.reward -> -1.000
[-] 2027 : Training/min.reward -> -1.000
[-] 2027 : Training/actions per episode -> 24.000
Q-Size: 1751
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 3, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 5, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 6, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 6, 1), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 6, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (5, 6, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 6, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 6, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 5, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2042 : Training/reward per episode -> 10.000
[-] 2042 : Training/max.reward -> 24.000
[-] 2042 : Training/min.reward -> -1.000
[-] 2042 : Training/actions per episode -> 14.000
Q-Size: 1766
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 4, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 4, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 5, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 5, 3), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 5, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 5, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 5, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 5, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 5, 0), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 5, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 5, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (6, 6, 2), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 1), (6, 6, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (5, 6, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (5, 6, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (5, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 6, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 6, 0), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2062 : Training/reward per episode -> 5.000
[-] 2062 : Training/max.reward -> 24.000
[-] 2062 : Training/min.reward -> -1.000
[-] 2062 : Training/actions per episode -> 19.000
Q-Size: 1785
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 4, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 3, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 2, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 1), (3, 2, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 2), (2, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 3), (2, 2, 0), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (2, 2, 1), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (2, 2, 2), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (2, 2, 3), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.90208
[ACT] ((6, 6, 3), (2, 2, 0), (2, 3, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((6, 6, 2), (2, 2, 1), (2, 3, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((6, 6, 1), (2, 2, 2), (2, 3, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.3117312
[ACT] ((6, 6, 2), (2, 2, 3), (2, 3, 0), 2) -1.90208
Max action: 2
[UpdateQ]-Q[x]= -2.3977472
[-] 2087 : Training/reward per episode -> -25.000
[-] 2087 : Training/max.reward -> -1.000
[-] 2087 : Training/min.reward -> -1.000
[-] 2087 : Training/actions per episode -> 24.000
Q-Size: 1801
Random Choice
[UpdateQ]-Q[x]= 3.2
[-] 2088 : Training/reward per episode -> 4.000
[-] 2088 : Training/max.reward -> 4.000
[-] 2088 : Training/min.reward -> 4.000
[-] 2088 : Training/actions per episode -> 0.000
Q-Size: 1802
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 2, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (3, 2, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (3, 2, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 2, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (4, 2, 1), (5, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 2), (4, 2, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 2, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 3), (4, 2, 0), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 2), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 2, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 2, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 3, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 5, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 5, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2113 : Training/reward per episode -> -25.000
[-] 2113 : Training/max.reward -> -1.000
[-] 2113 : Training/min.reward -> -1.000
[-] 2113 : Training/actions per episode -> 24.000
Q-Size: 1825
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 2, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 3, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 3, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 6, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 6, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (5, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 5, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 4, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 0), (5, 4, 3), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 1), (4, 4, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 4, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 5, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 5, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 5, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 5, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 5, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 4, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2138 : Training/reward per episode -> -25.000
[-] 2138 : Training/max.reward -> -1.000
[-] 2138 : Training/min.reward -> -1.000
[-] 2138 : Training/actions per episode -> 24.000
Q-Size: 1849
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 6, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 6, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 1), (4, 6, 3), (4, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 6, 1), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 6, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 6, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 6, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 2), (4, 6, 0), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (4, 6, 0), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 0), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 6, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 6, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2163 : Training/reward per episode -> -25.000
[-] 2163 : Training/max.reward -> -1.000
[-] 2163 : Training/min.reward -> -1.000
[-] 2163 : Training/actions per episode -> 24.000
Q-Size: 1871
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 6, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 1), (4, 6, 1), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 0), (4, 6, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 6, 0), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 6, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 6, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 3), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 6, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 6, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 6, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 6, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 6, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (2, 6, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2188 : Training/reward per episode -> -25.000
[-] 2188 : Training/max.reward -> -1.000
[-] 2188 : Training/min.reward -> -1.000
[-] 2188 : Training/actions per episode -> 24.000
Q-Size: 1895
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (5, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (6, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 2195 : Training/reward per episode -> 19.000
[-] 2195 : Training/max.reward -> 25.000
[-] 2195 : Training/min.reward -> -1.000
[-] 2195 : Training/actions per episode -> 6.000
Q-Size: 1902
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 5, 0), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 4, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 2), (2, 4, 0), (2, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 2, 1), (2, 4, 2), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 2), (2, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 4, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (4, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2220 : Training/reward per episode -> -25.000
[-] 2220 : Training/max.reward -> -1.000
[-] 2220 : Training/min.reward -> -1.000
[-] 2220 : Training/actions per episode -> 24.000
Q-Size: 1925
Random Choice
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (3, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 2), (3, 4, 3), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 1), (2, 4, 3), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((5, 6, 0), (2, 4, 3), (2, 5, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.0624
[ACT] ((5, 6, 1), (2, 4, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -2.27939131392
[ACT] ((6, 6, 1), (2, 4, 2), (2, 5, 0), 2) -2.311548928
Max action: 2
[UpdateQ]-Q[x]= -2.2043897856
[ACT] ((6, 6, 2), (2, 4, 3), (2, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.7088
[ACT] ((6, 6, 3), (2, 4, 0), (2, 5, 0), 0) -0.96
Max action: 0
[UpdateQ]-Q[x]= -1.504
[ACT] ((5, 6, 3), (2, 4, 1), (2, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 4, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.74208
[ACT] ((3, 6, 3), (2, 4, 1), (2, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((3, 6, 2), (2, 4, 2), (2, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.7088
[ACT] ((3, 6, 1), (2, 4, 3), (2, 5, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((3, 6, 2), (2, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 3), (2, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 6, 0), (2, 4, 1), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 3), (2, 4, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2240 : Training/reward per episode -> 5.000
[-] 2240 : Training/max.reward -> 24.000
[-] 2240 : Training/min.reward -> -1.000
[-] 2240 : Training/actions per episode -> 19.000
Q-Size: 1931
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (5, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 2, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 3, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 3, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 0), (4, 4, 1), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 3), (5, 4, 1), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 0), (5, 4, 2), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 1), (5, 4, 3), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 2), (5, 4, 0), (6, 4, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 6, 2), (5, 4, 1), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 1), (5, 4, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 4, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 4, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 2255 : Training/reward per episode -> 11.000
[-] 2255 : Training/max.reward -> 25.000
[-] 2255 : Training/min.reward -> -1.000
[-] 2255 : Training/actions per episode -> 14.000
Q-Size: 1940
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (3, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.4144
[ACT] ((4, 6, 2), (5, 4, 1), (6, 4, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -0.992
[ACT] ((4, 6, 1), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 4, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 4, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 2), (6, 4, 3), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 2), (6, 4, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 3), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 3), (6, 4, 3), (6, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 3), (6, 4, 0), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 4, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 4, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 4, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 4, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2280 : Training/reward per episode -> -25.000
[-] 2280 : Training/max.reward -> -1.000
[-] 2280 : Training/min.reward -> -1.000
[-] 2280 : Training/actions per episode -> 24.000
Q-Size: 1960
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (3, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 6, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 6, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 6, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 4, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 3, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 0), (2, 3, 1), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 5, 3), (2, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 2), (2, 3, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 5, 1), (2, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 2), (2, 3, 2), (2, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (2, 3, 3), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 1), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 1), (2, 3, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 0), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2305 : Training/reward per episode -> -25.000
[-] 2305 : Training/max.reward -> -1.000
[-] 2305 : Training/min.reward -> -1.000
[-] 2305 : Training/actions per episode -> 24.000
Q-Size: 1980
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 6, 3), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 6, 3), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 6, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (5, 6, 1), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (5, 6, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (6, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (6, 6, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (6, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (6, 6, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (6, 6, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 2), (6, 6, 0), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 3, 3), (6, 6, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (5, 6, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (5, 6, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (5, 6, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (6, 6, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (6, 6, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 6, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 6, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2330 : Training/reward per episode -> -25.000
[-] 2330 : Training/max.reward -> -1.000
[-] 2330 : Training/min.reward -> -1.000
[-] 2330 : Training/actions per episode -> 24.000
Q-Size: 2004
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 3, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 2, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (5, 2, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 2, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (3, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 2, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 3, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 5, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 6, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 6, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 6, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 5, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 3, 0), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 2, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 2, 1), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (3, 2, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (3, 2, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (3, 2, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2355 : Training/reward per episode -> -25.000
[-] 2355 : Training/max.reward -> -1.000
[-] 2355 : Training/min.reward -> -1.000
[-] 2355 : Training/actions per episode -> 24.000
Q-Size: 2029
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 4, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 2, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 1), (4, 2, 1), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 1), (4, 2, 3), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 2), (4, 2, 0), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((2, 6, 1), (4, 2, 1), (3, 2, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((2, 6, 2), (4, 2, 2), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.90208
[ACT] ((2, 6, 1), (4, 2, 3), (3, 2, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.102016
[ACT] ((2, 6, 2), (4, 2, 0), (3, 2, 0), 1) -1.5744
Max action: 1
[UpdateQ]-Q[x]= -2.07744
[ACT] ((2, 6, 1), (4, 2, 1), (3, 2, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= -1.1008
[ACT] ((2, 6, 2), (4, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 2, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 2, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 0), (3, 2, 1), (2, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 0), (3, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2380 : Training/reward per episode -> -25.000
[-] 2380 : Training/max.reward -> -1.000
[-] 2380 : Training/min.reward -> -1.000
[-] 2380 : Training/actions per episode -> 24.000
Q-Size: 2045
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (5, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 3, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 4, 0), (6, 4, 2), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 3), (6, 4, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 0), (6, 4, 0), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 1), (6, 4, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.90208
[ACT] ((2, 4, 0), (6, 4, 2), (6, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((2, 4, 3), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 2392 : Training/reward per episode -> -7.000
[-] 2392 : Training/max.reward -> 4.000
[-] 2392 : Training/min.reward -> -1.000
[-] 2392 : Training/actions per episode -> 11.000
Q-Size: 2052
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (3, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (3, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (3, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (3, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (3, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (3, 4, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 4, 0), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 3), (5, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 4, 0), (5, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 4, 1), (5, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 4, 2), (5, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 5, 2), (5, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 5, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 5, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 5, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 2, 1), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2417 : Training/reward per episode -> -25.000
[-] 2417 : Training/max.reward -> -1.000
[-] 2417 : Training/min.reward -> -1.000
[-] 2417 : Training/actions per episode -> 24.000
Q-Size: 2077
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (3, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 2419 : Training/reward per episode -> 3.000
[-] 2419 : Training/max.reward -> 4.000
[-] 2419 : Training/min.reward -> -1.000
[-] 2419 : Training/actions per episode -> 1.000
Q-Size: 2079
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 3, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 3, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 3, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 3, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 3, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.74208
[ACT] ((6, 4, 3), (4, 2, 0), (3, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((6, 4, 2), (4, 2, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 2, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 1), (4, 2, 0), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 2), (3, 2, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (3, 2, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (5, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2444 : Training/reward per episode -> -25.000
[-] 2444 : Training/max.reward -> -1.000
[-] 2444 : Training/min.reward -> -1.000
[-] 2444 : Training/actions per episode -> 24.000
Q-Size: 2102
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 3, 2), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 3, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 2, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 2, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (3, 2, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (3, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (4, 3, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 3, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 1.248
[ACT] ((2, 4, 3), (4, 4, 3), (5, 4, 0), 0) 3.2
Max action: 0
[UpdateQ]-Q[x]= 5.888
[-] 2455 : Training/reward per episode -> -6.000
[-] 2455 : Training/max.reward -> 4.000
[-] 2455 : Training/min.reward -> -1.000
[-] 2455 : Training/actions per episode -> 10.000
Q-Size: 2112
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 6, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 6, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 6, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 5, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 5, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 5, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 5, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 0), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 4, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 4, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 4, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 3, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 2480 : Training/reward per episode -> -20.000
[-] 2480 : Training/max.reward -> 4.000
[-] 2480 : Training/min.reward -> -1.000
[-] 2480 : Training/actions per episode -> 24.000
Q-Size: 2137
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 6, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 6, 1), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 6, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 3), (6, 4, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= 19.328
[-] 2494 : Training/reward per episode -> 12.000
[-] 2494 : Training/max.reward -> 25.000
[-] 2494 : Training/min.reward -> -1.000
[-] 2494 : Training/actions per episode -> 13.000
Q-Size: 2150
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 5, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (6, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 5, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (6, 5, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 4, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 2, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 0.0
[-] 2506 : Training/reward per episode -> -11.000
[-] 2506 : Training/max.reward -> 0.000
[-] 2506 : Training/min.reward -> -1.000
[-] 2506 : Training/actions per episode -> 11.000
Q-Size: 2162
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (2, 3, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 19.2
[-] 2508 : Training/reward per episode -> 23.000
[-] 2508 : Training/max.reward -> 24.000
[-] 2508 : Training/min.reward -> -1.000
[-] 2508 : Training/actions per episode -> 1.000
Q-Size: 2164
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 3, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 2, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 2, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 2, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (5, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (5, 2, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 2), (5, 2, 1), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 2518 : Training/reward per episode -> -10.000
[-] 2518 : Training/max.reward -> -1.000
[-] 2518 : Training/min.reward -> -1.000
[-] 2518 : Training/actions per episode -> 9.000
Q-Size: 2173
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 2, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 2), (4, 2, 1), (5, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 3, 3), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (6, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (6, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 0), (6, 2, 1), (5, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 3, 1), (6, 3, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (6, 3, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (6, 3, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (6, 3, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (6, 2, 0), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (6, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 0.0
[-] 2531 : Training/reward per episode -> -12.000
[-] 2531 : Training/max.reward -> 0.000
[-] 2531 : Training/min.reward -> -1.000
[-] 2531 : Training/actions per episode -> 12.000
Q-Size: 2184
Random Choice
[UpdateQ]-Q[x]= 20.0
[-] 2532 : Training/reward per episode -> 25.000
[-] 2532 : Training/max.reward -> 25.000
[-] 2532 : Training/min.reward -> 25.000
[-] 2532 : Training/actions per episode -> 0.000
Q-Size: 2186
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 2, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 2, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (3, 2, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (3, 2, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 3, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (4, 3, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 3, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2542 : Training/reward per episode -> -10.000
[-] 2542 : Training/max.reward -> -1.000
[-] 2542 : Training/min.reward -> -1.000
[-] 2542 : Training/actions per episode -> 9.000
Q-Size: 2196
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 3), (4, 4, 1), (5, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 3, 0), (4, 5, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 5, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 5, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (3, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 0.0
[-] 2553 : Training/reward per episode -> -10.000
[-] 2553 : Training/max.reward -> 0.000
[-] 2553 : Training/min.reward -> -1.000
[-] 2553 : Training/actions per episode -> 10.000
Q-Size: 2206
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 3), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 2, 3), (3, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2563 : Training/reward per episode -> -10.000
[-] 2563 : Training/max.reward -> -1.000
[-] 2563 : Training/min.reward -> -1.000
[-] 2563 : Training/actions per episode -> 9.000
Q-Size: 2216
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 3, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 3, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 1), (6, 3, 0), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 0), (6, 3, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 3, 3), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 3, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 3, 2), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 3, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 3, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 3, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 3, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 3, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 3, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 3, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 3, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (6, 3, 2), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 0), (6, 3, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 1), (6, 3, 1), (6, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 0), (6, 3, 2), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 1), (6, 3, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 3, 3), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 3, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (6, 3, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 3, 2), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2588 : Training/reward per episode -> -25.000
[-] 2588 : Training/max.reward -> -1.000
[-] 2588 : Training/min.reward -> -1.000
[-] 2588 : Training/actions per episode -> 24.000
Q-Size: 2237
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (3, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (2, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (2, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 6, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 0), (2, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 1), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 6, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 6, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 2), (2, 6, 3), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 3, 1), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 3, 2), (2, 6, 2), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 2), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 2), (2, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 3), (2, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.4144
[ACT] ((5, 4, 3), (2, 6, 1), (3, 6, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.6064
[-] 2613 : Training/reward per episode -> -25.000
[-] 2613 : Training/max.reward -> -1.000
[-] 2613 : Training/min.reward -> -1.000
[-] 2613 : Training/actions per episode -> 24.000
Q-Size: 2257
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (3, 4, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 2), (3, 4, 3), (2, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (2, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2619 : Training/reward per episode -> -6.000
[-] 2619 : Training/max.reward -> -1.000
[-] 2619 : Training/min.reward -> -1.000
[-] 2619 : Training/actions per episode -> 5.000
Q-Size: 2262
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 2, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 3, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 3, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (5, 4, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (5, 4, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 4, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (5, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (5, 4, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2631 : Training/reward per episode -> -12.000
[-] 2631 : Training/max.reward -> -1.000
[-] 2631 : Training/min.reward -> -1.000
[-] 2631 : Training/actions per episode -> 11.000
Q-Size: 2274
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 4, 3), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 4, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 3), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 4, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 4, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (3, 2, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (3, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (3, 2, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (3, 2, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (3, 2, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 2), (3, 2, 0), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 3), (3, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (3, 2, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (3, 2, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (3, 2, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (3, 2, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (3, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2656 : Training/reward per episode -> -25.000
[-] 2656 : Training/max.reward -> -1.000
[-] 2656 : Training/min.reward -> -1.000
[-] 2656 : Training/actions per episode -> 24.000
Q-Size: 2298
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 5, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (2, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 2, 2), (2, 6, 3), (3, 6, 0), 0) -1.312
Max action: 0
[UpdateQ]-Q[x]= -1.6768
[ACT] ((6, 3, 2), (2, 6, 3), (3, 6, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((6, 3, 1), (2, 6, 1), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((6, 3, 2), (2, 6, 2), (3, 6, 0), 0) -0.96
Max action: 0
[UpdateQ]-Q[x]= -1.6064
[ACT] ((6, 4, 2), (2, 6, 2), (3, 6, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((6, 4, 3), (2, 6, 0), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 0), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((6, 4, 1), (2, 6, 2), (3, 6, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -2.02496
[ACT] ((6, 4, 2), (2, 6, 2), (3, 6, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= -1.7152
[ACT] ((6, 4, 3), (2, 6, 0), (3, 6, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -2.0413952
[ACT] ((6, 4, 0), (2, 6, 1), (3, 6, 0), 2) -1.63968
Max action: 2
[UpdateQ]-Q[x]= -1.127936
[ACT] ((6, 4, 1), (2, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 2), (2, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -2.266492928
[ACT] ((6, 4, 3), (2, 6, 0), (3, 6, 0), 2) -2.0413952
Max action: 2
[UpdateQ]-Q[x]= -1.72027904
[ACT] ((6, 4, 0), (2, 6, 0), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 4, 1), (2, 6, 1), (3, 6, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -2.51295547392
[ACT] ((6, 4, 2), (2, 6, 3), (3, 6, 0), 2) -2.266492928
Max action: 2
[UpdateQ]-Q[x]= -2.3542771712
[ACT] ((6, 4, 3), (2, 6, 0), (3, 6, 0), 2) -1.72027904
Max action: 2
[UpdateQ]-Q[x]= -1.865934848
[ACT] ((6, 4, 0), (2, 6, 1), (3, 6, 0), 2) -1.127936
Max action: 2
[UpdateQ]-Q[x]= -2.63387870331
[ACT] ((6, 4, 1), (2, 6, 1), (3, 6, 0), 2) -2.51295547392
Max action: 2
[UpdateQ]-Q[x]= -2.80932848435
[ACT] ((6, 4, 2), (2, 6, 3), (3, 6, 0), 2) -2.3542771712
Max action: 2
[UpdateQ]-Q[x]= -2.77759282381
[-] 2681 : Training/reward per episode -> -25.000
[-] 2681 : Training/max.reward -> -1.000
[-] 2681 : Training/min.reward -> -1.000
[-] 2681 : Training/actions per episode -> 24.000
Q-Size: 2303
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (3, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (3, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (3, 4, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 4, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 5, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 5, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 6, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 6, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 6, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((6, 6, 3), (4, 6, 3), (4, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((6, 6, 2), (4, 6, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 6, 1), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 6, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2706 : Training/reward per episode -> -25.000
[-] 2706 : Training/max.reward -> -1.000
[-] 2706 : Training/min.reward -> -1.000
[-] 2706 : Training/actions per episode -> 24.000
Q-Size: 2327
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 6, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (3, 6, 1), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 6, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (5, 6, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (5, 6, 3), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (5, 6, 3), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (5, 6, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 6, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (6, 5, 0), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 1), (6, 5, 0), (6, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= 11.328
[ACT] ((5, 2, 1), (6, 4, 1), (6, 3, 0), 0) 19.2
Max action: 0
[UpdateQ]-Q[x]= 35.328
[-] 2720 : Training/reward per episode -> 11.000
[-] 2720 : Training/max.reward -> 24.000
[-] 2720 : Training/min.reward -> -1.000
[-] 2720 : Training/actions per episode -> 13.000
Q-Size: 2338
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (5, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 2725 : Training/reward per episode -> 0.000
[-] 2725 : Training/max.reward -> 4.000
[-] 2725 : Training/min.reward -> -1.000
[-] 2725 : Training/actions per episode -> 4.000
Q-Size: 2343
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (5, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (5, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 0), (5, 2, 3), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 1), (5, 2, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (5, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (5, 2, 3), (2, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 2), (5, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (5, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 2, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (5, 2, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (5, 2, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 2, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 2, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (6, 2, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (6, 2, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (6, 2, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 2, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 2, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 2, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 2), (4, 2, 2), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 1), (4, 4, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2750 : Training/reward per episode -> -25.000
[-] 2750 : Training/max.reward -> -1.000
[-] 2750 : Training/min.reward -> -1.000
[-] 2750 : Training/actions per episode -> 24.000
Q-Size: 2365
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (3, 4, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 4, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 0), (4, 4, 3), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 1), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 2, 2), (4, 4, 1), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 2, 3), (4, 4, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.4144
[ACT] ((3, 2, 0), (4, 4, 3), (4, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -0.992
[ACT] ((3, 2, 1), (4, 4, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2760 : Training/reward per episode -> 15.000
[-] 2760 : Training/max.reward -> 24.000
[-] 2760 : Training/min.reward -> -1.000
[-] 2760 : Training/actions per episode -> 9.000
Q-Size: 2372
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 6, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 6, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 6, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (6, 6, 2), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (5, 6, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 6, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (6, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (6, 6, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 0), (6, 6, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 1), (6, 6, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 6, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 0), (6, 6, 1), (2, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 0), (6, 6, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 6, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 4, 0), (6, 6, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 4, 1), (6, 6, 1), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 2), (6, 6, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 6, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.63968
[ACT] ((2, 4, 2), (6, 6, 1), (2, 2, 0), 0) -1.312
Max action: 0
[UpdateQ]-Q[x]= -1.90208
[-] 2785 : Training/reward per episode -> -25.000
[-] 2785 : Training/max.reward -> -1.000
[-] 2785 : Training/min.reward -> -1.000
[-] 2785 : Training/actions per episode -> 24.000
Q-Size: 2391
Random Choice
[UpdateQ]-Q[x]= -0.8
[-] 2786 : Training/reward per episode -> -1.000
[-] 2786 : Training/max.reward -> -1.000
[-] 2786 : Training/min.reward -> -1.000
[-] 2786 : Training/actions per episode -> 0.000
Q-Size: 2392
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 5, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 5, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 5, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 5, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 5, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 6, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 6, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 6, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 6, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 5, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 5, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 5, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 5, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 5, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 5, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 5, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 5, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 5, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 5, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 1), (2, 5, 1), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 0), (2, 5, 2), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 5, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 5, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 5, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2811 : Training/reward per episode -> -25.000
[-] 2811 : Training/max.reward -> -1.000
[-] 2811 : Training/min.reward -> -1.000
[-] 2811 : Training/actions per episode -> 24.000
Q-Size: 2416
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 4, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 4, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 4, 1), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 4, 2), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (5, 4, 3), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 4, 0), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 4, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 2820 : Training/reward per episode -> 17.000
[-] 2820 : Training/max.reward -> 25.000
[-] 2820 : Training/min.reward -> -1.000
[-] 2820 : Training/actions per episode -> 8.000
Q-Size: 2425
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (2, 5, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (2, 5, 0), (4, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (2, 4, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2824 : Training/reward per episode -> -4.000
[-] 2824 : Training/max.reward -> -1.000
[-] 2824 : Training/min.reward -> -1.000
[-] 2824 : Training/actions per episode -> 3.000
Q-Size: 2428
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (5, 6, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (5, 6, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 6, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (5, 6, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (5, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (5, 6, 3), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (5, 6, 2), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (5, 6, 3), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (5, 6, 0), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (5, 6, 0), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 6, 2), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (5, 6, 1), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (5, 6, 0), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 2), (5, 6, 3), (7, 4, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 2), (4, 6, 3), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 6, 3), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 6, 3), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 6, 3), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 6, 2), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 6, 1), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 6, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 2849 : Training/reward per episode -> -25.000
[-] 2849 : Training/max.reward -> -1.000
[-] 2849 : Training/min.reward -> -1.000
[-] 2849 : Training/actions per episode -> 24.000
Q-Size: 2452
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 2, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 3, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 3, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 3, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 3, 1), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 3, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 3, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 3, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 4, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 4, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2864 : Training/reward per episode -> 10.000
[-] 2864 : Training/max.reward -> 24.000
[-] 2864 : Training/min.reward -> -1.000
[-] 2864 : Training/actions per episode -> 14.000
Q-Size: 2467
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 3, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 3, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 3, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 3, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 3, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 2, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (6, 2, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 3), (6, 2, 0), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 0), (6, 2, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (6, 2, 0), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 3), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (6, 2, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (6, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (5, 2, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2889 : Training/reward per episode -> -25.000
[-] 2889 : Training/max.reward -> -1.000
[-] 2889 : Training/min.reward -> -1.000
[-] 2889 : Training/actions per episode -> 24.000
Q-Size: 2488
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (3, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (3, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (3, 4, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (3, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 1), (3, 4, 3), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 0), (3, 4, 0), (2, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 2, 1), (3, 4, 1), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 2, 0), (3, 4, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (3, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (3, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 2900 : Training/reward per episode -> 15.000
[-] 2900 : Training/max.reward -> 25.000
[-] 2900 : Training/min.reward -> -1.000
[-] 2900 : Training/actions per episode -> 10.000
Q-Size: 2496
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 2, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 2, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (3, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (3, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 2), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 2910 : Training/reward per episode -> 15.000
[-] 2910 : Training/max.reward -> 24.000
[-] 2910 : Training/min.reward -> -1.000
[-] 2910 : Training/actions per episode -> 9.000
Q-Size: 2506
[ACT] ((2, 5, 2), (4, 3, 3), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 3, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 5, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (4, 5, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= 19.328
[-] 2915 : Training/reward per episode -> 21.000
[-] 2915 : Training/max.reward -> 25.000
[-] 2915 : Training/min.reward -> -1.000
[-] 2915 : Training/actions per episode -> 4.000
Q-Size: 2509
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (5, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (3, 4, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 4, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (3, 4, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (3, 4, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (3, 4, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (3, 4, 0), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 0), (3, 4, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (3, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (5, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (5, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 3), (5, 4, 2), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 0), (5, 4, 1), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 1), (5, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 12.0
[ACT] ((6, 2, 3), (5, 4, 1), (6, 3, 0), 0) 20.0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[ACT] ((5, 2, 3), (5, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 2940 : Training/reward per episode -> -25.000
[-] 2940 : Training/max.reward -> -1.000
[-] 2940 : Training/min.reward -> -1.000
[-] 2940 : Training/actions per episode -> 24.000
Q-Size: 2530
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 2, 0), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 2), (4, 2, 2), (5, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 3), (4, 2, 3), (5, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 3), (4, 2, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 2, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (5, 2, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 2, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 2), (6, 2, 0), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (6, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 3), (6, 2, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((4, 6, 2), (6, 2, 0), (6, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -0.992
[ACT] ((4, 6, 3), (6, 2, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 2, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 2, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 3, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 4, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 2965 : Training/reward per episode -> -25.000
[-] 2965 : Training/max.reward -> -1.000
[-] 2965 : Training/min.reward -> -1.000
[-] 2965 : Training/actions per episode -> 24.000
Q-Size: 2550
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 2, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 2, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 2, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (3, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (2, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 2, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (2, 2, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 2, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (2, 3, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 3, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 2978 : Training/reward per episode -> 13.000
[-] 2978 : Training/max.reward -> 25.000
[-] 2978 : Training/min.reward -> -1.000
[-] 2978 : Training/actions per episode -> 12.000
Q-Size: 2563
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 5, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 5, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 5, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 5, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 5, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 5, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (2, 5, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 4, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (2, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 0), (3, 4, 1), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 2, 1), (3, 4, 1), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((6, 2, 0), (5, 4, 1), (6, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((6, 2, 1), (5, 4, 1), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= 19.328
[-] 2993 : Training/reward per episode -> 11.000
[-] 2993 : Training/max.reward -> 25.000
[-] 2993 : Training/min.reward -> -1.000
[-] 2993 : Training/actions per episode -> 14.000
Q-Size: 2574
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 6, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 6, 1), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 6, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 6, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (4, 5, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 5, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 3, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 2, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3004 : Training/reward per episode -> -6.000
[-] 3004 : Training/max.reward -> 4.000
[-] 3004 : Training/min.reward -> -1.000
[-] 3004 : Training/actions per episode -> 10.000
Q-Size: 2585
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (6, 6, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 5, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 4, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (6, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (5, 2, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 3018 : Training/reward per episode -> 12.000
[-] 3018 : Training/max.reward -> 25.000
[-] 3018 : Training/min.reward -> -1.000
[-] 3018 : Training/actions per episode -> 13.000
Q-Size: 2598
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 4, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 3, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 1), (6, 2, 0), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((2, 3, 2), (6, 2, 0), (6, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.83168
[ACT] ((2, 3, 3), (6, 2, 1), (6, 3, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.2346752
[ACT] ((2, 3, 2), (6, 2, 0), (6, 3, 0), 2) -1.83168
Max action: 2
[UpdateQ]-Q[x]= -1.166336
[ACT] ((2, 3, 3), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 0), (6, 2, 3), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 3, 1), (4, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 2, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (5, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (5, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (5, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (5, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 3, 2), (4, 2, 0), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 4, 2), (4, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 4, 3), (4, 2, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 4, 0), (4, 2, 0), (2, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((2, 4, 3), (4, 2, 0), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3042 : Training/reward per episode -> -19.000
[-] 3042 : Training/max.reward -> 4.000
[-] 3042 : Training/min.reward -> -1.000
[-] 3042 : Training/actions per episode -> 23.000
Q-Size: 2614
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 5, 1), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 5, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 5, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 5, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 5, 0), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 5, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 6, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 6, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 6, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 6, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 6, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 3), (2, 6, 3), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 2), (2, 6, 0), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 1), (2, 6, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 6, 1), (2, 6, 0), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 0), (2, 6, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 6, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 6, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 6, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 6, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 2), (2, 6, 2), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[-] 3067 : Training/reward per episode -> -25.000
[-] 3067 : Training/max.reward -> -1.000
[-] 3067 : Training/min.reward -> -1.000
[-] 3067 : Training/actions per episode -> 24.000
Q-Size: 2635
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 3), (2, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 4, 3), (2, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (2, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 4, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 4, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 4, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 0), (2, 4, 0), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 5, 1), (2, 3, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((6, 5, 0), (2, 3, 1), (2, 2, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((6, 5, 3), (2, 3, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 5, 0), (2, 3, 3), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((6, 5, 3), (2, 3, 3), (2, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((6, 5, 2), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 1), (2, 3, 2), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 5, 0), (2, 3, 2), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 5, 3), (2, 3, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((6, 5, 2), (2, 3, 0), (2, 2, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.93408
[ACT] ((6, 5, 1), (2, 3, 2), (2, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((6, 5, 0), (2, 3, 2), (2, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.102016
[ACT] ((6, 5, 3), (2, 3, 0), (2, 2, 0), 1) -1.5744
Max action: 1
[UpdateQ]-Q[x]= -1.95456
[ACT] ((6, 5, 2), (2, 3, 1), (2, 2, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.3657472
[ACT] ((6, 5, 1), (2, 3, 2), (2, 2, 0), 1) -2.03648
Max action: 1
[UpdateQ]-Q[x]= -2.3590912
[ACT] ((6, 5, 0), (2, 3, 3), (2, 2, 0), 1) -1.79968
Max action: 1
[UpdateQ]-Q[x]= -2.3117312
[-] 3092 : Training/reward per episode -> -25.000
[-] 3092 : Training/max.reward -> -1.000
[-] 3092 : Training/min.reward -> -1.000
[-] 3092 : Training/actions per episode -> 24.000
Q-Size: 2644
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 2, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 2, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 2, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (2, 2, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 2, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 0), (2, 2, 0), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 1), (2, 2, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 1), (2, 2, 3), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 0), (2, 2, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 2, 2), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 2, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (3, 2, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (3, 2, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (3, 2, 2), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (3, 2, 2), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (3, 2, 2), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 2, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 2, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 2, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (3, 2, 0), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 0), (3, 2, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3117 : Training/reward per episode -> -25.000
[-] 3117 : Training/max.reward -> -1.000
[-] 3117 : Training/min.reward -> -1.000
[-] 3117 : Training/actions per episode -> 24.000
Q-Size: 2666
[ACT] ((2, 6, 3), (4, 3, 0), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 3, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 3, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (3, 4, 3), (1, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (3, 4, 3), (1, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 3123 : Training/reward per episode -> 20.000
[-] 3123 : Training/max.reward -> 25.000
[-] 3123 : Training/min.reward -> -1.000
[-] 3123 : Training/actions per episode -> 5.000
Q-Size: 2671
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 2, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (2, 3, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 4, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 4, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 4, 1), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (3, 4, 1), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (3, 4, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (3, 4, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (3, 4, 1), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (3, 4, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (3, 4, 3), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 4, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (3, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 0), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 4, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 5, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (4, 5, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (4, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (5, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (5, 6, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3148 : Training/reward per episode -> -25.000
[-] 3148 : Training/max.reward -> -1.000
[-] 3148 : Training/min.reward -> -1.000
[-] 3148 : Training/actions per episode -> 24.000
Q-Size: 2696
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 3, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 5, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (4, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (4, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.74208
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((2, 3, 1), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.500416
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) -1.0944
Max action: 1
[UpdateQ]-Q[x]= -1.53088
[ACT] ((2, 3, 1), (4, 6, 1), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 3, 2), (4, 6, 2), (5, 6, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((2, 3, 1), (4, 6, 3), (5, 6, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.0741632
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) -1.53088
Max action: 1
[UpdateQ]-Q[x]= -2.2579712
[ACT] ((2, 3, 1), (4, 6, 1), (5, 6, 0), 2) -1.79968
Max action: 2
[UpdateQ]-Q[x]= -2.4428032
[ACT] ((2, 3, 2), (4, 6, 2), (5, 6, 0), 1) -2.00448
Max action: 1
[UpdateQ]-Q[x]= -2.528360448
[ACT] ((2, 3, 1), (4, 6, 3), (5, 6, 0), 2) -2.0741632
Max action: 2
[UpdateQ]-Q[x]= -2.659934208
[ACT] ((2, 3, 2), (4, 6, 0), (5, 6, 0), 1) -2.2579712
Max action: 1
[UpdateQ]-Q[x]= -2.814988288
[ACT] ((2, 3, 1), (4, 6, 1), (5, 6, 0), 2) -2.4428032
Max action: 2
[UpdateQ]-Q[x]= -1.28856064
[ACT] ((2, 3, 2), (5, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 5, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3173 : Training/reward per episode -> -25.000
[-] 3173 : Training/max.reward -> -1.000
[-] 3173 : Training/min.reward -> -1.000
[-] 3173 : Training/actions per episode -> 24.000
Q-Size: 2710
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 3, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 3, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 3, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (6, 3, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (6, 4, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 4, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 5, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 5, 1), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 5, 1), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 5, 1), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (6, 5, 1), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 2), (6, 5, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 5, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 2), (6, 5, 2), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 3), (6, 6, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 6, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 6, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (6, 6, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (6, 6, 3), (7, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 6, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 6, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 5, 3), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3198 : Training/reward per episode -> -25.000
[-] 3198 : Training/max.reward -> -1.000
[-] 3198 : Training/min.reward -> -1.000
[-] 3198 : Training/actions per episode -> 24.000
Q-Size: 2733
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 3, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 3, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 4, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (5, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (5, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 4, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (6, 4, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (6, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (6, 4, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (5, 4, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (5, 4, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 4, 0), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (4, 3, 0), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 3, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 3, 2), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 3, 3), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 3, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 3, 1), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 1), (4, 3, 2), (4, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[-] 3223 : Training/reward per episode -> -25.000
[-] 3223 : Training/max.reward -> -1.000
[-] 3223 : Training/min.reward -> -1.000
[-] 3223 : Training/actions per episode -> 24.000
Q-Size: 2757
[ACT] ((6, 2, 2), (4, 2, 2), (5, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 3), (4, 3, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 2), (4, 4, 2), (5, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 3), (4, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 11.56992
[ACT] ((6, 2, 1), (5, 4, 1), (6, 3, 0), 1) 19.328
Max action: 1
[UpdateQ]-Q[x]= 36.23552
[-] 3229 : Training/reward per episode -> 20.000
[-] 3229 : Training/max.reward -> 25.000
[-] 3229 : Training/min.reward -> -1.000
[-] 3229 : Training/actions per episode -> 5.000
Q-Size: 2760
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 5, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 5, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 5, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 5, 0), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 5, 0), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 3236 : Training/reward per episode -> 19.000
[-] 3236 : Training/max.reward -> 25.000
[-] 3236 : Training/min.reward -> -1.000
[-] 3236 : Training/actions per episode -> 6.000
Q-Size: 2767
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 4, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 2, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 2, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 2, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 3, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 3, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 3, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 3252 : Training/reward per episode -> 10.000
[-] 3252 : Training/max.reward -> 25.000
[-] 3252 : Training/min.reward -> -1.000
[-] 3252 : Training/actions per episode -> 15.000
Q-Size: 2783
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (5, 6, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 5, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 4, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 4, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (5, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.94568331264
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) -1.790130176
Max action: 1
[UpdateQ]-Q[x]= -2.72490790912
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -2.448252928
Max action: 2
[UpdateQ]-Q[x]= -2.5249517568
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -1.93015808
Max action: 1
[UpdateQ]-Q[x]= -2.6180390912
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) -2.23751168
Max action: 2
[UpdateQ]-Q[x]= -2.99144339784
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) -2.72490790912
Max action: 1
[UpdateQ]-Q[x]= -2.96095070618
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -2.5249517568
Max action: 2
[UpdateQ]-Q[x]= -2.98053536973
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -2.6180390912
Max action: 1
[UpdateQ]-Q[x]= -3.23813159286
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) -2.99144339784
Max action: 2
[UpdateQ]-Q[x]= -2.68115587957
[ACT] ((2, 2, 0), (6, 4, 0), (6, 5, 0), 1) -2.00448
Max action: 1
[UpdateQ]-Q[x]= -3.10843863663
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -2.98053536973
Max action: 2
[UpdateQ]-Q[x]= -3.46851129337
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -3.23813159286
Max action: 1
[UpdateQ]-Q[x]= -3.16356608149
[ACT] ((2, 2, 3), (6, 4, 0), (6, 5, 0), 2) -2.68115587957
Max action: 2
[UpdateQ]-Q[x]= -3.23123962787
[ACT] ((2, 2, 0), (6, 4, 1), (6, 5, 0), 1) -2.96095070618
Max action: 1
[UpdateQ]-Q[x]= -3.61203736899
[ACT] ((2, 2, 3), (6, 4, 2), (6, 5, 0), 2) -3.46851129337
Max action: 2
[UpdateQ]-Q[x]= -3.51838455083
[ACT] ((2, 2, 0), (6, 4, 3), (6, 5, 0), 1) -3.16356608149
Max action: 1
[UpdateQ]-Q[x]= -3.45739550846
[-] 3277 : Training/reward per episode -> -25.000
[-] 3277 : Training/max.reward -> -1.000
[-] 3277 : Training/min.reward -> -1.000
[-] 3277 : Training/actions per episode -> 24.000
Q-Size: 2793
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 2), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 2, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (3, 2, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (3, 2, 3), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 3), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (2, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (2, 2, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 2, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (2, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 4, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 4, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 3, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 3, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3302 : Training/reward per episode -> -25.000
[-] 3302 : Training/max.reward -> -1.000
[-] 3302 : Training/min.reward -> -1.000
[-] 3302 : Training/actions per episode -> 24.000
Q-Size: 2818
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 6, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 2), (2, 6, 1), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 3), (2, 6, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 3), (2, 6, 0), (3, 6, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 4, 3), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 0), (2, 6, 2), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 4, 1), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 6, 0), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (2, 5, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (2, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (2, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (5, 4, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3317 : Training/reward per episode -> -10.000
[-] 3317 : Training/max.reward -> 4.000
[-] 3317 : Training/min.reward -> -1.000
[-] 3317 : Training/actions per episode -> 14.000
Q-Size: 2830
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (3, 2, 0), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (3, 2, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (3, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 2, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 3, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 3, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 3, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (2, 4, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 5, 2), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 5, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 5, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 5, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 5, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (2, 5, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (2, 4, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (2, 4, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (1, 4, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 0.0
[-] 3336 : Training/reward per episode -> -18.000
[-] 3336 : Training/max.reward -> 0.000
[-] 3336 : Training/min.reward -> -1.000
[-] 3336 : Training/actions per episode -> 18.000
Q-Size: 2849
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (4, 2, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 2, 0), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (5, 2, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (5, 2, 2), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 2, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 3), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 2, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 2, 3), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 2), (5, 2, 1), (6, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 2, 3), (5, 2, 2), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 2, 2), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 2, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (4, 3, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 3, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (4, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (4, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (3, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 3, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 3, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3361 : Training/reward per episode -> -25.000
[-] 3361 : Training/max.reward -> -1.000
[-] 3361 : Training/min.reward -> -1.000
[-] 3361 : Training/actions per episode -> 24.000
Q-Size: 2873
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (2, 2, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 2, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 2, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 2, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 2, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (2, 2, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (2, 2, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (2, 2, 0), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 2, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 2, 1), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (2, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 2, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 2, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 0), (2, 2, 1), (2, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 0), (2, 2, 1), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (2, 2, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (3, 2, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (4, 2, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 2, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 2, 3), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3386 : Training/reward per episode -> -25.000
[-] 3386 : Training/max.reward -> -1.000
[-] 3386 : Training/min.reward -> -1.000
[-] 3386 : Training/actions per episode -> 24.000
Q-Size: 2897
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (6, 5, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 5, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 5, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 3), (6, 5, 1), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 4, 2), (6, 5, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 5, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 5, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (6, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (5, 4, 3), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 4, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 3400 : Training/reward per episode -> 11.000
[-] 3400 : Training/max.reward -> 24.000
[-] 3400 : Training/min.reward -> -1.000
[-] 3400 : Training/actions per episode -> 13.000
Q-Size: 2910
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (5, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 20.0
[-] 3406 : Training/reward per episode -> 20.000
[-] 3406 : Training/max.reward -> 25.000
[-] 3406 : Training/min.reward -> -1.000
[-] 3406 : Training/actions per episode -> 5.000
Q-Size: 2916
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 3, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 3, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 3, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 3, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 3, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 3, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 3, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (4, 3, 2), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 1), (4, 3, 3), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 3, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 3, 3), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 3, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (4, 3, 3), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.4144
[ACT] ((2, 6, 3), (4, 3, 3), (2, 2, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -0.992
[ACT] ((2, 6, 2), (4, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 3, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 6, 1), (4, 3, 1), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 6, 2), (4, 3, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3431 : Training/reward per episode -> -25.000
[-] 3431 : Training/max.reward -> -1.000
[-] 3431 : Training/min.reward -> -1.000
[-] 3431 : Training/actions per episode -> 24.000
Q-Size: 2937
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 6, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (6, 6, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 3), (6, 6, 1), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 2), (6, 6, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 6, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (6, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 6, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 6, 1), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 6, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 5, 0), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (5, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 4, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 4, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 5, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 5, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (4, 5, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 5, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 5, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (4, 5, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (4, 5, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 5, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 5, 3), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3456 : Training/reward per episode -> -25.000
[-] 3456 : Training/max.reward -> -1.000
[-] 3456 : Training/min.reward -> -1.000
[-] 3456 : Training/actions per episode -> 24.000
Q-Size: 2961
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 4, 1), (4, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (2, 4, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (2, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (2, 2, 2), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (2, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (2, 2, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (2, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (2, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 2), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 3), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (2, 2, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 0), (2, 2, 2), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 5, 1), (2, 2, 3), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 5, 0), (2, 2, 3), (3, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 5, 3), (2, 2, 1), (3, 2, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.0624
[ACT] ((2, 5, 0), (2, 2, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (2, 2, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (2, 2, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3481 : Training/reward per episode -> -25.000
[-] 3481 : Training/max.reward -> -1.000
[-] 3481 : Training/min.reward -> -1.000
[-] 3481 : Training/actions per episode -> 24.000
Q-Size: 2982
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 5, 2), (6, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 6, 2), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 6, 1), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (3, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 2), (3, 6, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 1), (5, 6, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.4144
[ACT] ((6, 4, 2), (5, 6, 1), (6, 5, 0), 0) -0.96
Max action: 0
[UpdateQ]-Q[x]= 19.1936
[-] 3489 : Training/reward per episode -> 18.000
[-] 3489 : Training/max.reward -> 25.000
[-] 3489 : Training/min.reward -> -1.000
[-] 3489 : Training/actions per episode -> 7.000
Q-Size: 2988
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 2, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (5, 2, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 2, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (4, 2, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (4, 3, 2), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 4, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (4, 4, 2), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 4, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 5, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 5, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 2), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 3), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 2), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3514 : Training/reward per episode -> -25.000
[-] 3514 : Training/max.reward -> -1.000
[-] 3514 : Training/min.reward -> -1.000
[-] 3514 : Training/actions per episode -> 24.000
Q-Size: 3013
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 2, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 2, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (6, 2, 2), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (6, 3, 2), (7, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 3, 2), (7, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 3521 : Training/reward per episode -> 19.000
[-] 3521 : Training/max.reward -> 25.000
[-] 3521 : Training/min.reward -> -1.000
[-] 3521 : Training/actions per episode -> 6.000
Q-Size: 3020
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (2, 6, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (2, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (3, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 1), (5, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 6, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 6, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (6, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (6, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (5, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 3536 : Training/reward per episode -> 10.000
[-] 3536 : Training/max.reward -> 24.000
[-] 3536 : Training/min.reward -> -1.000
[-] 3536 : Training/actions per episode -> 14.000
Q-Size: 3035
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (3, 2, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (3, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (3, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 2, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 3, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 3, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 4, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 3), (2, 4, 3), (3, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 5, 2), (2, 4, 0), (3, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 5, 3), (2, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 4, 2), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.74208
[ACT] ((6, 5, 3), (2, 4, 3), (3, 4, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((6, 5, 2), (2, 4, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (2, 4, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 4, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 2, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (2, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (3, 2, 1), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (3, 2, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (3, 2, 3), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3561 : Training/reward per episode -> -25.000
[-] 3561 : Training/max.reward -> -1.000
[-] 3561 : Training/min.reward -> -1.000
[-] 3561 : Training/actions per episode -> 24.000
Q-Size: 3057
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (5, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 1), (4, 4, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 4, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 4, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 0), (4, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 5, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 6, 2), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 6, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (5, 6, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 6, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 6, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (6, 6, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (6, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 6, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 6, 3), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (4, 6, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (4, 6, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (4, 6, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (4, 6, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 2, 1), (4, 6, 2), (6, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((3, 2, 1), (4, 6, 1), (4, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((3, 2, 2), (4, 6, 0), (4, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.6064
[-] 3586 : Training/reward per episode -> -25.000
[-] 3586 : Training/max.reward -> -1.000
[-] 3586 : Training/min.reward -> -1.000
[-] 3586 : Training/actions per episode -> 24.000
Q-Size: 3079
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 0), (5, 4, 2), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 1), (5, 4, 2), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (5, 4, 0), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (5, 4, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 4, 3), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (4, 4, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (4, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 4, 1), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (4, 4, 2), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 4, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 4, 1), (4, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (4, 4, 3), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (4, 4, 0), (4, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 4, 1), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 4, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 3, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 3), (4, 2, 0), (3, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 3), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.4144
[ACT] ((4, 4, 2), (4, 2, 2), (3, 2, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -1.504
[ACT] ((4, 4, 1), (4, 2, 2), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 4, 2), (4, 2, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3611 : Training/reward per episode -> -25.000
[-] 3611 : Training/max.reward -> -1.000
[-] 3611 : Training/min.reward -> -1.000
[-] 3611 : Training/actions per episode -> 24.000
Q-Size: 3101
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (2, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 4, 1), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 2), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (4, 4, 1), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 0), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (5, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (5, 4, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (5, 4, 3), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.76256
[ACT] ((6, 2, 0), (5, 4, 1), (6, 3, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= 22.0899328
[ACT] ((6, 2, 1), (5, 4, 1), (6, 3, 0), 1) 36.23552
Max action: 1
[UpdateQ]-Q[x]= 50.4378368
[-] 3623 : Training/reward per episode -> 14.000
[-] 3623 : Training/max.reward -> 25.000
[-] 3623 : Training/min.reward -> -1.000
[-] 3623 : Training/actions per episode -> 11.000
Q-Size: 3110
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (2, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (2, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (3, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 2), (4, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (5, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.807616
[ACT] ((4, 4, 3), (5, 4, 2), (6, 4, 0), 1) -1.5744
Max action: 1
[UpdateQ]-Q[x]= -2.07744
[ACT] ((4, 4, 2), (5, 4, 3), (6, 4, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= -1.6128
[ACT] ((4, 4, 3), (5, 4, 3), (6, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.5744
[ACT] ((4, 4, 2), (5, 4, 1), (6, 4, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -0.992
[ACT] ((4, 4, 1), (5, 4, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (5, 4, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (4, 4, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (3, 4, 3), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 4, 3), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 4, 3), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 3, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 3, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 2, 1), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (3, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (3, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (4, 2, 0), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (4, 2, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3648 : Training/reward per episode -> -25.000
[-] 3648 : Training/max.reward -> -1.000
[-] 3648 : Training/min.reward -> -1.000
[-] 3648 : Training/actions per episode -> 24.000
Q-Size: 3131
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (3, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 3), (4, 6, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 0), (5, 6, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 3), (5, 6, 1), (6, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 3), (6, 6, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 2), (6, 6, 3), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 6, 1), (6, 6, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 6, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (6, 6, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 5, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 6, 1), (6, 4, 0), (6, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 6, 1), (6, 4, 0), (6, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.90208
[ACT] ((6, 6, 1), (6, 4, 2), (6, 3, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((6, 6, 0), (6, 4, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (6, 4, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (6, 4, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.500416
[ACT] ((6, 6, 1), (6, 4, 2), (6, 3, 0), 1) -1.0944
Max action: 1
[UpdateQ]-Q[x]= -1.53088
[ACT] ((6, 6, 0), (6, 4, 2), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 6, 1), (6, 4, 0), (6, 3, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.90208
[ACT] ((6, 6, 2), (6, 4, 1), (6, 3, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.0741632
[ACT] ((6, 6, 1), (6, 4, 2), (6, 3, 0), 1) -1.53088
Max action: 1
[UpdateQ]-Q[x]= -1.618176
[ACT] ((6, 6, 0), (6, 4, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 6, 3), (6, 4, 0), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.74208
[ACT] ((5, 6, 3), (6, 4, 0), (6, 3, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[-] 3673 : Training/reward per episode -> -25.000
[-] 3673 : Training/max.reward -> -1.000
[-] 3673 : Training/min.reward -> -1.000
[-] 3673 : Training/actions per episode -> 24.000
Q-Size: 3142
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 2, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (2, 4, 0), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 4, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 3679 : Training/reward per episode -> 19.000
[-] 3679 : Training/max.reward -> 24.000
[-] 3679 : Training/min.reward -> -1.000
[-] 3679 : Training/actions per episode -> 5.000
Q-Size: 3148
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (5, 6, 0), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 6, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (5, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (5, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (5, 6, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (5, 6, 1), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 6, 3), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 6, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (5, 6, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (5, 6, 2), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (5, 6, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 0), (5, 6, 0), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 1), (5, 6, 1), (4, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 2), (5, 6, 2), (4, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 2, 1), (5, 6, 3), (4, 6, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -2.00448
[ACT] ((4, 2, 0), (5, 6, 0), (4, 6, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.03648
[ACT] ((4, 2, 1), (5, 6, 1), (4, 6, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 2, 2), (5, 6, 1), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (5, 6, 2), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (5, 6, 3), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (5, 6, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (5, 6, 1), (4, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3704 : Training/reward per episode -> -25.000
[-] 3704 : Training/max.reward -> -1.000
[-] 3704 : Training/min.reward -> -1.000
[-] 3704 : Training/actions per episode -> 24.000
Q-Size: 3167
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 5, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 5, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (4, 5, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 5, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 5, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 5, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (4, 5, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (4, 5, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (4, 5, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 5, 3), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 5, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (4, 6, 2), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (4, 6, 2), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 6, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 6, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 6, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 6, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (3, 6, 2), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (3, 6, 3), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (3, 6, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (3, 6, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3728 : Training/reward per episode -> -19.000
[-] 3728 : Training/max.reward -> 4.000
[-] 3728 : Training/min.reward -> -1.000
[-] 3728 : Training/actions per episode -> 23.000
Q-Size: 3190
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (5, 4, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (5, 4, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 4, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 0), (4, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 4, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 5, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 3), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 6, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 3), (4, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 2), (4, 6, 0), (3, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 1), (4, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 6, 2), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 1), (4, 6, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 5, 2), (4, 6, 3), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 5, 1), (4, 6, 1), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 5, 2), (4, 6, 2), (3, 6, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.6768
[ACT] ((2, 5, 1), (4, 6, 3), (3, 6, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.6064
[ACT] ((2, 5, 2), (4, 6, 0), (3, 6, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -2.1437952
[ACT] ((2, 5, 1), (4, 6, 1), (3, 6, 0), 2) -1.79968
Max action: 2
[UpdateQ]-Q[x]= -2.233088
[ACT] ((2, 5, 2), (4, 6, 2), (3, 6, 0), 1) -1.6768
Max action: 1
[UpdateQ]-Q[x]= -1.13536
[ACT] ((2, 5, 1), (4, 6, 2), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -2.172028928
[ACT] ((2, 5, 2), (4, 6, 0), (3, 6, 0), 1) -2.1437952
Max action: 1
[UpdateQ]-Q[x]= -2.600787968
[-] 3753 : Training/reward per episode -> -25.000
[-] 3753 : Training/max.reward -> -1.000
[-] 3753 : Training/min.reward -> -1.000
[-] 3753 : Training/actions per episode -> 24.000
Q-Size: 3206
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 3), (6, 2, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 0), (6, 2, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 0), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 1), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 0), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (6, 4, 3), (6, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 1), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -2.65190847611
[ACT] ((6, 2, 1), (6, 4, 0), (6, 5, 0), 1) -2.89360699392
Max action: 1
[UpdateQ]-Q[x]= -1.89072139878
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -2.281506816
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -2.3148544
Max action: 2
[UpdateQ]-Q[x]= -1.26297088
[ACT] ((6, 2, 1), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.4144
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -2.45216436224
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) -2.281506816
Max action: 2
[UpdateQ]-Q[x]= -1.2563013632
[ACT] ((6, 2, 0), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 3), (6, 4, 0), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -2.52938519183
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) -2.45216436224
Max action: 1
[UpdateQ]-Q[x]= -2.0944657449
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) -1.2563013632
Max action: 2
[UpdateQ]-Q[x]= -1.85956163584
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -1.26297088
Max action: 2
[UpdateQ]-Q[x]= -1.957810176
[ACT] ((6, 2, 1), (6, 4, 1), (6, 5, 0), 1) -1.4144
Max action: 1
[UpdateQ]-Q[x]= -2.42333807673
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) -2.0944657449
Max action: 1
[UpdateQ]-Q[x]= -2.40901259592
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) -1.85956163584
Max action: 2
[UpdateQ]-Q[x]= -2.36203177411
[-] 3778 : Training/reward per episode -> -25.000
[-] 3778 : Training/max.reward -> -1.000
[-] 3778 : Training/min.reward -> -1.000
[-] 3778 : Training/actions per episode -> 24.000
Q-Size: 3218
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 2, 0), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (5, 2, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 2, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (4, 2, 3), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (4, 3, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 4, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 3785 : Training/reward per episode -> 19.000
[-] 3785 : Training/max.reward -> 25.000
[-] 3785 : Training/min.reward -> -1.000
[-] 3785 : Training/actions per episode -> 6.000
Q-Size: 3225
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 2, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 2, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 2, 1), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (5, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (5, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 2, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (6, 2, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (6, 2, 0), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (6, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (6, 2, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 2, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (6, 2, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (6, 2, 2), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 3), (6, 3, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 0), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 2), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 2), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3810 : Training/reward per episode -> -25.000
[-] 3810 : Training/max.reward -> -1.000
[-] 3810 : Training/min.reward -> -1.000
[-] 3810 : Training/actions per episode -> 24.000
Q-Size: 3249
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 1), (3, 4, 3), (3, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 2), (2, 4, 3), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 4, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (2, 3, 0), (6, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (2, 3, 1), (6, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (2, 3, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 3, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (2, 3, 1), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (2, 3, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (2, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (2, 2, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (2, 2, 0), (4, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (2, 2, 3), (4, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3825 : Training/reward per episode -> -10.000
[-] 3825 : Training/max.reward -> 4.000
[-] 3825 : Training/min.reward -> -1.000
[-] 3825 : Training/actions per episode -> 14.000
Q-Size: 3263
Random Choice
[UpdateQ]-Q[x]= 19.2
[-] 3826 : Training/reward per episode -> 24.000
[-] 3826 : Training/max.reward -> 24.000
[-] 3826 : Training/min.reward -> 24.000
[-] 3826 : Training/actions per episode -> 0.000
Q-Size: 3265
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (5, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 3), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 0), (6, 4, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 3), (6, 4, 2), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 2), (6, 4, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 3, 3), (6, 4, 0), (6, 5, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.0624
[ACT] ((4, 3, 0), (6, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 0), (6, 4, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((4, 2, 1), (6, 4, 3), (6, 5, 0), 0) -0.96
Max action: 0
[UpdateQ]-Q[x]= -2.68922142471
[ACT] ((5, 2, 1), (6, 4, 3), (6, 5, 0), 0) -2.65190847611
Max action: 0
[UpdateQ]-Q[x]= -2.54044339044
[ACT] ((6, 2, 1), (6, 4, 0), (6, 5, 0), 1) -1.89072139878
Max action: 1
[UpdateQ]-Q[x]= -2.84444505211
[ACT] ((6, 2, 0), (6, 4, 1), (6, 5, 0), 2) -2.6035949568
Max action: 2
[UpdateQ]-Q[x]= -2.66686119936
[ACT] ((6, 2, 1), (6, 4, 3), (6, 5, 0), 1) -2.1033472
Max action: 1
[UpdateQ]-Q[x]= -2.06034944
[ACT] ((6, 2, 0), (6, 4, 0), (6, 5, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((6, 2, 3), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (6, 4, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -2.31170033543
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) -2.36203177411
Max action: 2
[UpdateQ]-Q[x]= -2.52540486746
[ACT] ((6, 2, 0), (6, 4, 3), (6, 5, 0), 2) -1.957810176
Max action: 2
[UpdateQ]-Q[x]= -2.74249840431
[ACT] ((6, 2, 1), (6, 4, 1), (6, 5, 0), 1) -2.42333807673
Max action: 1
[UpdateQ]-Q[x]= -2.82643567673
[ACT] ((6, 2, 0), (6, 4, 2), (6, 5, 0), 1) -2.40901259592
Max action: 1
[UpdateQ]-Q[x]= -2.89806163436
[ACT] ((6, 2, 3), (6, 4, 3), (6, 5, 0), 2) -2.52540486746
Max action: 2
[UpdateQ]-Q[x]= -2.92134008867
[-] 3851 : Training/reward per episode -> -25.000
[-] 3851 : Training/max.reward -> -1.000
[-] 3851 : Training/min.reward -> -1.000
[-] 3851 : Training/actions per episode -> 24.000
Q-Size: 3274
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 4, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.74208
[ACT] ((4, 2, 0), (2, 4, 3), (2, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -2.03648
[ACT] ((4, 2, 1), (2, 4, 0), (2, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 2, 2), (2, 4, 2), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 4, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 4, 3), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (2, 4, 1), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 2), (2, 4, 2), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 1), (2, 4, 3), (2, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 2, 0), (2, 4, 0), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.90208
[ACT] ((4, 2, 1), (2, 4, 1), (2, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.6064
[ACT] ((4, 2, 2), (2, 4, 1), (2, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 3, 2), (2, 4, 3), (2, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.9397632
[ACT] ((4, 3, 3), (2, 4, 0), (2, 5, 0), 1) -1.53088
Max action: 1
[UpdateQ]-Q[x]= -2.048256
[ACT] ((4, 3, 2), (2, 4, 0), (2, 5, 0), 0) -1.472
Max action: 0
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 4, 2), (2, 4, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 5, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 5, 1), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 5, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 4, 1), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 4, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[-] 3876 : Training/reward per episode -> -25.000
[-] 3876 : Training/max.reward -> -1.000
[-] 3876 : Training/min.reward -> -1.000
[-] 3876 : Training/actions per episode -> 24.000
Q-Size: 3288
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 4, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 4, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 3, 1), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (2, 3, 0), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 3, 3), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 3, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (2, 3, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 2), (2, 3, 0), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 1), (2, 3, 1), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 0), (2, 3, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.79968
[ACT] ((4, 5, 1), (2, 3, 3), (2, 2, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -2.00448
[ACT] ((4, 5, 2), (2, 3, 0), (2, 2, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -1.0944
[ACT] ((4, 5, 1), (2, 3, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (2, 3, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -2.0828672
[ACT] ((4, 5, 1), (2, 3, 3), (2, 2, 0), 2) -2.00448
Max action: 2
[UpdateQ]-Q[x]= -1.200896
[ACT] ((4, 5, 2), (2, 3, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 3), (2, 3, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 0), (2, 3, 1), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[-] 3901 : Training/reward per episode -> -25.000
[-] 3901 : Training/max.reward -> -1.000
[-] 3901 : Training/min.reward -> -1.000
[-] 3901 : Training/actions per episode -> 24.000
Q-Size: 3306
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (2, 5, 1), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (2, 5, 1), (4, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 6, 1), (2, 4, 0), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 6, 2), (2, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 2), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 3), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 0), (2, 4, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 4, 1), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 3), (2, 4, 0), (5, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 3), (2, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (3, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 4, 1), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (5, 4, 1), (6, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (5, 4, 1), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 1), (6, 4, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 2), (6, 5, 2), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (6, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 20.0
[-] 3921 : Training/reward per episode -> 6.000
[-] 3921 : Training/max.reward -> 25.000
[-] 3921 : Training/min.reward -> -1.000
[-] 3921 : Training/actions per episode -> 19.000
Q-Size: 3325
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (3, 4, 2), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (3, 4, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 2), (4, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 4, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 3, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 2, 1), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.9517952
[ACT] ((6, 4, 2), (4, 2, 2), (3, 2, 0), 1) -1.79968
Max action: 1
[UpdateQ]-Q[x]= -1.159936
[ACT] ((6, 4, 1), (4, 2, 3), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 2, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 3, 2), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 2), (4, 3, 3), (4, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (4, 3, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 3935 : Training/reward per episode -> -9.000
[-] 3935 : Training/max.reward -> 4.000
[-] 3935 : Training/min.reward -> -1.000
[-] 3935 : Training/actions per episode -> 13.000
Q-Size: 3338
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (5, 2, 2), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (5, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 3939 : Training/reward per episode -> 21.000
[-] 3939 : Training/max.reward -> 24.000
[-] 3939 : Training/min.reward -> -1.000
[-] 3939 : Training/actions per episode -> 3.000
Q-Size: 3342
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (3, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 6, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 6, 3), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 3, 1), (2, 6, 3), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 3, 0), (2, 6, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 6, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 6, 0), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 6, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (2, 6, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 1), (2, 6, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (2, 6, 0), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (2, 6, 1), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 6, 1), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 6, 1), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 6, 2), (2, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 6, 2), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (3, 6, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 2, 1), (3, 6, 2), (2, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 2, 0), (3, 6, 3), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.4144
[ACT] ((5, 2, 1), (3, 6, 2), (2, 2, 0), 1) -0.96
Max action: 1
[UpdateQ]-Q[x]= -0.992
[ACT] ((5, 2, 0), (3, 6, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (3, 6, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (3, 6, 1), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 6, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 3964 : Training/reward per episode -> -25.000
[-] 3964 : Training/max.reward -> -1.000
[-] 3964 : Training/min.reward -> -1.000
[-] 3964 : Training/actions per episode -> 24.000
Q-Size: 3364
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 3, 1), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (2, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 2), (2, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 1), (2, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (4, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (5, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 2, 1), (6, 4, 2), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 2, 2), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 2), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 4, 2), (6, 4, 2), (6, 5, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((4, 5, 2), (6, 4, 3), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 3), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((4, 5, 0), (6, 4, 1), (6, 5, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((4, 5, 1), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 5, 0), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 1), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 0), (6, 4, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 4, 3), (6, 4, 2), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -1.312
[ACT] ((3, 4, 3), (6, 4, 0), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((3, 4, 2), (6, 4, 0), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 4, 3), (6, 4, 3), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[-] 3989 : Training/reward per episode -> -25.000
[-] 3989 : Training/max.reward -> -1.000
[-] 3989 : Training/min.reward -> -1.000
[-] 3989 : Training/actions per episode -> 24.000
Q-Size: 3384
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 4, 0), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 4, 0), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 3), (4, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (4, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (4, 4, 3), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 4, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 4, 1), (4, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 2), (4, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 4, 0), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (4, 4, 1), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (4, 4, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 1), (4, 4, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 0), (4, 4, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 2, 3), (4, 4, 1), (4, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 2, 2), (4, 4, 1), (5, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 3), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 1), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 1), (4, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 0), (4, 4, 3), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 0), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 2), (4, 4, 0), (5, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 3, 3), (4, 4, 2), (5, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 4014 : Training/reward per episode -> -25.000
[-] 4014 : Training/max.reward -> -1.000
[-] 4014 : Training/min.reward -> -1.000
[-] 4014 : Training/actions per episode -> 24.000
Q-Size: 3408
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (5, 2, 2), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (6, 2, 1), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 2, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 2, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 0), (6, 4, 3), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (6, 4, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (6, 4, 1), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (6, 4, 2), (6, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.74208
[ACT] ((5, 6, 2), (6, 4, 3), (6, 5, 0), 1) -1.472
Max action: 1
[UpdateQ]-Q[x]= -2.03648
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) -1.472
Max action: 2
[UpdateQ]-Q[x]= -1.7088
[ACT] ((5, 6, 2), (6, 4, 1), (6, 5, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((5, 6, 3), (6, 4, 1), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -2.2633472
[ACT] ((5, 6, 2), (6, 4, 3), (6, 5, 0), 1) -2.03648
Max action: 1
[UpdateQ]-Q[x]= -2.300928
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) -1.7088
Max action: 2
[UpdateQ]-Q[x]= -2.10432
[ACT] ((5, 6, 2), (6, 4, 1), (6, 5, 0), 2) -1.504
Max action: 2
[UpdateQ]-Q[x]= -2.549342208
[ACT] ((5, 6, 3), (6, 4, 1), (6, 5, 0), 1) -2.2633472
Max action: 1
[UpdateQ]-Q[x]= -2.72526336
[ACT] ((5, 6, 2), (6, 4, 3), (6, 5, 0), 1) -2.300928
Max action: 1
[UpdateQ]-Q[x]= -2.6069504
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) -2.10432
Max action: 2
[UpdateQ]-Q[x]= -1.732864
[ACT] ((5, 6, 2), (6, 4, 0), (6, 5, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -2.0749312
[ACT] ((5, 6, 1), (6, 4, 2), (6, 5, 0), 2) -1.74208
Max action: 2
[UpdateQ]-Q[x]= -1.148416
[ACT] ((5, 6, 2), (6, 4, 2), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.90903296
[ACT] ((5, 6, 1), (6, 4, 0), (6, 5, 0), 2) -1.732864
Max action: 2
[UpdateQ]-Q[x]= -2.474528768
[ACT] ((5, 6, 2), (6, 4, 0), (6, 5, 0), 1) -2.0749312
Max action: 1
[UpdateQ]-Q[x]= -2.542942208
[-] 4039 : Training/reward per episode -> -25.000
[-] 4039 : Training/max.reward -> -1.000
[-] 4039 : Training/min.reward -> -1.000
[-] 4039 : Training/actions per episode -> 24.000
Q-Size: 3419
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 3), (4, 5, 1), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (4, 5, 1), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 5, 0), (6, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 1), (4, 5, 1), (6, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 5, 2), (4, 5, 1), (6, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 5, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (4, 5, 3), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 6, 0), (4, 5, 0), (3, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 1), (4, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 4, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 4, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 3, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 3), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (3, 2, 3), (2, 4, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 2), (3, 2, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (3, 2, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (3, 2, 1), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (5, 2, 1), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (5, 2, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 1), (5, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 2), (4, 2, 3), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 3), (4, 2, 0), (6, 4, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -0.96
[ACT] ((2, 6, 0), (4, 2, 1), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 4064 : Training/reward per episode -> -25.000
[-] 4064 : Training/max.reward -> -1.000
[-] 4064 : Training/min.reward -> -1.000
[-] 4064 : Training/actions per episode -> 24.000
Q-Size: 3440
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 2, 3), (4, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 2), (4, 2, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 1), (4, 3, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 6, 0), (4, 3, 2), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 20.0
[-] 4069 : Training/reward per episode -> 21.000
[-] 4069 : Training/max.reward -> 25.000
[-] 4069 : Training/min.reward -> -1.000
[-] 4069 : Training/actions per episode -> 4.000
Q-Size: 3445
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (6, 2, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 2, 1), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 3), (6, 2, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (6, 2, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 2, 2), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (6, 3, 2), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 3.2
[-] 4076 : Training/reward per episode -> -2.000
[-] 4076 : Training/max.reward -> 4.000
[-] 4076 : Training/min.reward -> -1.000
[-] 4076 : Training/actions per episode -> 6.000
Q-Size: 3452
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (6, 4, 3), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 1), (6, 4, 3), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (6, 4, 1), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (6, 4, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 1), (6, 4, 3), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 0), (6, 4, 0), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((6, 4, 3), (6, 4, 1), (6, 3, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -1.472
[ACT] ((5, 4, 3), (6, 4, 2), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((5, 4, 2), (6, 4, 2), (6, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((5, 4, 1), (6, 4, 3), (4, 2, 0), 0) -0.8
Max action: 0
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 1), (5, 4, 3), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 0), (4, 4, 2), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 4, 3), (4, 5, 2), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 4, 0), (4, 6, 2), (5, 6, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.96
[ACT] ((6, 4, 3), (4, 6, 3), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 3), (4, 6, 0), (5, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 4, 0), (4, 6, 1), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.43488
[ACT] ((5, 4, 3), (4, 6, 2), (5, 6, 0), 2) -0.992
Max action: 2
[UpdateQ]-Q[x]= -1.5104
[ACT] ((5, 4, 0), (4, 6, 3), (5, 6, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -2.2428672
[ACT] ((5, 4, 1), (4, 6, 0), (5, 6, 0), 2) -2.00448
Max action: 2
[UpdateQ]-Q[x]= -2.294528
[ACT] ((5, 4, 2), (4, 6, 1), (5, 6, 0), 2) -1.7088
Max action: 2
[UpdateQ]-Q[x]= -2.235392
[-] 4101 : Training/reward per episode -> -25.000
[-] 4101 : Training/max.reward -> -1.000
[-] 4101 : Training/min.reward -> -1.000
[-] 4101 : Training/actions per episode -> 24.000
Q-Size: 3468
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 3, 0), (3, 2, 0), (3, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 2, 1), (3, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (3, 2, 0), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 0), (3, 2, 1), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (3, 2, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (3, 2, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (3, 2, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 2, 1), (2, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 2, 1), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (4, 2, 2), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (4, 2, 1), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (5, 2, 0), (5, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 0), (6, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 2, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 3), (5, 2, 0), (6, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 2, 2), (5, 2, 1), (6, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 3, 2), (6, 2, 1), (6, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.63968
[ACT] ((2, 3, 3), (6, 2, 0), (6, 3, 0), 2) -1.312
Max action: 2
[UpdateQ]-Q[x]= -1.6768
[ACT] ((2, 3, 0), (6, 2, 3), (6, 3, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -1.504
[ACT] ((2, 3, 1), (6, 2, 3), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.472
[ACT] ((2, 3, 0), (6, 2, 2), (6, 3, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -1.79968
[ACT] ((2, 3, 3), (6, 2, 3), (6, 3, 0), 1) -1.312
Max action: 1
[UpdateQ]-Q[x]= -1.0624
[ACT] ((2, 3, 2), (6, 2, 3), (3, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 4, 2), (6, 2, 3), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[-] 4126 : Training/reward per episode -> -25.000
[-] 4126 : Training/max.reward -> -1.000
[-] 4126 : Training/min.reward -> -1.000
[-] 4126 : Training/actions per episode -> 24.000
Q-Size: 3488
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 2), (2, 5, 3), (4, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 6, 1), (2, 5, 3), (3, 2, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 19.2
[-] 4129 : Training/reward per episode -> 22.000
[-] 4129 : Training/max.reward -> 24.000
[-] 4129 : Training/min.reward -> -1.000
[-] 4129 : Training/actions per episode -> 2.000
Q-Size: 3491
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (5, 2, 2), (2, 4, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 1), (4, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 2), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 6, 3), (4, 2, 2), (4, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 0), (4, 2, 2), (5, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((2, 6, 3), (4, 2, 2), (5, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -1.312
[ACT] ((2, 6, 0), (4, 2, 2), (5, 2, 0), 1) -0.8
Max action: 1
[UpdateQ]-Q[x]= -0.672
[-] 4138 : Training/reward per episode -> -8.000
[-] 4138 : Training/max.reward -> 0.000
[-] 4138 : Training/min.reward -> -1.000
[-] 4138 : Training/actions per episode -> 8.000
Q-Size: 3499
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (3, 4, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 4, 0), (2, 3, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 4, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 3, 0), (3, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 0), (4, 3, 0), (2, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (4, 3, 0), (5, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 2), (4, 3, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 1), (4, 3, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= 0.0
[-] 4147 : Training/reward per episode -> -8.000
[-] 4147 : Training/max.reward -> 0.000
[-] 4147 : Training/min.reward -> -1.000
[-] 4147 : Training/actions per episode -> 8.000
Q-Size: 3508
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 2, 3), (2, 3, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 2, 3), (3, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 1), (2, 4, 2), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 2), (2, 4, 2), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -1.312
[ACT] ((6, 5, 1), (2, 4, 2), (2, 2, 0), 2) -0.8
Max action: 2
[UpdateQ]-Q[x]= -1.5744
[ACT] ((6, 5, 2), (2, 4, 0), (3, 4, 0), 2) -0.96
Max action: 2
[UpdateQ]-Q[x]= -0.992
[ACT] ((6, 5, 3), (2, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -2.0378112
[ACT] ((6, 5, 2), (2, 3, 0), (2, 2, 0), 1) -1.93408
Max action: 1
[UpdateQ]-Q[x]= -1.186816
[ACT] ((6, 5, 1), (2, 3, 0), (2, 5, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 5, 0), (2, 3, 0), (2, 5, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 0.0
[-] 4158 : Training/reward per episode -> -10.000
[-] 4158 : Training/max.reward -> 0.000
[-] 4158 : Training/min.reward -> -1.000
[-] 4158 : Training/actions per episode -> 10.000
Q-Size: 3516
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 2), (2, 5, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 2, 3), (2, 6, 0), (6, 5, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((5, 2, 3), (2, 6, 0), (3, 6, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 6, 0), (3, 6, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 1), (2, 6, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 2), (2, 6, 0), (6, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((4, 2, 3), (2, 6, 0), (2, 3, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= -0.8
[ACT] ((3, 2, 3), (2, 5, 0), (2, 4, 0), 0) 0
Max action: 0
[UpdateQ]-Q[x]= 0.0
[-] 4168 : Training/reward per episode -> -9.000
[-] 4168 : Training/max.reward -> 0.000
[-] 4168 : Training/min.reward -> -1.000
[-] 4168 : Training/actions per episode -> 9.000
Q-Size: 3526
Random Choice
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (5, 4, 0), (2, 4, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (4, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (3, 4, 0), (2, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 2), (3, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 4, 0), (2, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 3, 0), (3, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 3, 0), (3, 2, 0), 1) 0
Max action: 1
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 0), (2, 2, 0), (4, 2, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= -0.8
[ACT] ((6, 6, 1), (2, 2, 0), (3, 6, 0), 2) 0
Max action: 2
[UpdateQ]-Q[x]= 0.0
[-] 4178 : Training/reward per episode -> -9.000
[-] 4178 : Training/max.reward -> 0.000
[-] 4178 : Training/min.reward -> -1.000
[-] 4178 : Training/actions per episode -> 9.000
